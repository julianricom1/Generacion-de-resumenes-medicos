{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merge de LoRA y Subida a S3\n",
        "\n",
        "Este notebook mergea un modelo LoRA con su modelo base y sube el resultado a S3.\n",
        "\n",
        "## Configuración\n",
        "\n",
        "1. Asegúrate de tener instaladas las dependencias:\n",
        "   ```bash\n",
        "   pip install torch transformers peft huggingface-hub boto3\n",
        "   ```\n",
        "\n",
        "2. Configura el token de Hugging Face en `KEYS.py` (en la raíz del proyecto)\n",
        "\n",
        "3. Configura tus credenciales AWS (para subir a S3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.6.0+cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers version: 4.45.0\n",
            "PEFT version: 0.10.0\n",
            "Todas las dependencias están disponibles.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Agregar el directorio raíz al path para importar KEYS\n",
        "sys.path.insert(0, str(Path().absolute().parent))\n",
        "\n",
        "try:\n",
        "    from KEYS import HF_TOKEN, MODEL_S3_BUCKET\n",
        "except ImportError:\n",
        "    print(\"ERROR: No se encontró KEYS.py. Crea el archivo con HF_TOKEN y MODEL_S3_BUCKET\")\n",
        "    HF_TOKEN = None\n",
        "    MODEL_S3_BUCKET = \"modelo-generador-maia-g8\"\n",
        "\n",
        "try:\n",
        "    import boto3\n",
        "    S3_AVAILABLE = True\n",
        "except ImportError:\n",
        "    S3_AVAILABLE = False\n",
        "    print(\"WARNING: boto3 no está instalado. No se podrá subir a S3.\")\n",
        "    print(\"Instala con: pip install boto3\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR: torch no está instalado. Instala con: pip install torch\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "    import transformers\n",
        "    print(f\"Transformers version: {transformers.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR: transformers no está instalado.\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    from peft import PeftModel\n",
        "    import peft\n",
        "    print(f\"PEFT version: {peft.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR: peft no está instalado.\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import login\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR: huggingface_hub no está instalado.\")\n",
        "    raise\n",
        "\n",
        "print(\"Todas las dependencias están disponibles.\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuración de Parámetros\n",
        "\n",
        "Ajusta estos valores según tu caso:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LORA_PATH: generacion/ollama/outputs/meta-llama__Llama-3.2-3B-Instruct-6_epocas/final\n",
            "MODEL_S3_BUCKET: modelo-generador-maia-g8\n",
            "AWS_REGION: us-east-1\n",
            "MODEL_NAME detectado automáticamente: meta-llama__Llama-3.2-3B-Instruct-6_epocas\n"
          ]
        }
      ],
      "source": [
        "# Ruta al directorio final del entrenamiento LoRA\n",
        "LORA_PATH = \"generacion/ollama/outputs/meta-llama__Llama-3.2-3B-Instruct-6_epocas/final\"\n",
        "\n",
        "# Nombre del modelo (se detecta automáticamente del LORA_PATH si no se especifica)\n",
        "MODEL_NAME = None  # Si es None, se usa el nombre del directorio padre de LORA_PATH\n",
        "\n",
        "# Región de AWS\n",
        "AWS_REGION = \"us-east-1\"\n",
        "\n",
        "print(f\"LORA_PATH: {LORA_PATH}\")\n",
        "print(f\"MODEL_S3_BUCKET: {MODEL_S3_BUCKET}\")\n",
        "print(f\"AWS_REGION: {AWS_REGION}\")\n",
        "\n",
        "# Detectar MODEL_NAME si no se especificó\n",
        "if MODEL_NAME is None:\n",
        "    MODEL_NAME = Path(LORA_PATH).parent.name\n",
        "    print(f\"MODEL_NAME detectado automáticamente: {MODEL_NAME}\")\n",
        "else:\n",
        "    print(f\"MODEL_NAME: {MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Función de Merge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_lora_with_base(lora_dir: str, output_dir: str):\n",
        "    \"\"\"Mergea el LoRA con el modelo base\"\"\"\n",
        "    print(f\"Mergeando LoRA desde {lora_dir}...\")\n",
        "    \n",
        "    # Autenticarse con Hugging Face si hay token disponible\n",
        "    if HF_TOKEN:\n",
        "        print(\"Autenticándose con Hugging Face...\")\n",
        "        try:\n",
        "            login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "            print(\"Autenticación exitosa con Hugging Face\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: No se pudo autenticar con Hugging Face: {e}\")\n",
        "    else:\n",
        "        print(\"WARNING: No se encontró HF_TOKEN en KEYS.py\")\n",
        "    \n",
        "    # Resolver el path (puede ser relativo o absoluto)\n",
        "    lora_path = Path(lora_dir)\n",
        "    if not lora_path.is_absolute():\n",
        "        # Si es relativo, intentar desde el directorio raíz del proyecto\n",
        "        project_root = Path().absolute().parent\n",
        "        lora_path = project_root / lora_dir\n",
        "        print(f\"Path relativo detectado, resolviendo desde raíz del proyecto: {lora_path}\")\n",
        "    \n",
        "    # Verificar que el directorio existe\n",
        "    if not lora_path.exists():\n",
        "        print(f\"ERROR: El directorio no existe: {lora_path}\")\n",
        "        print(f\"Directorio actual de trabajo: {Path.cwd()}\")\n",
        "        print(f\"Intentando path absoluto: {Path(lora_dir).absolute()}\")\n",
        "        raise FileNotFoundError(f\"No se encontró el directorio: {lora_path}\")\n",
        "    \n",
        "    cfg_path = lora_path / \"adapter_config.json\"\n",
        "    \n",
        "    if not cfg_path.exists():\n",
        "        print(f\"ERROR: No se encontró adapter_config.json\")\n",
        "        print(f\"Directorio buscado: {lora_path}\")\n",
        "        print(f\"Archivos en el directorio:\")\n",
        "        for item in lora_path.iterdir():\n",
        "            print(f\"  - {item.name} ({'dir' if item.is_dir() else 'file'})\")\n",
        "        raise FileNotFoundError(f\"No se encontró adapter_config.json en {lora_path}\")\n",
        "    \n",
        "    adapter_cfg = json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n",
        "    base_model_name = adapter_cfg.get(\"base_model_name_or_path\")\n",
        "    \n",
        "    if not base_model_name:\n",
        "        raise ValueError(\"adapter_config.json no contiene 'base_model_name_or_path'\")\n",
        "    \n",
        "    print(f\"Modelo base: {base_model_name}\")\n",
        "    print(f\"Directorio de salida: {output_dir}\")\n",
        "    \n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(\"Cargando tokenizer...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=False, trust_remote_code=True)\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Error al cargar tokenizer desde modelo base: {e}\")\n",
        "        print(\"Intentando cargar desde directorio LoRA...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(str(lora_path), use_fast=False, trust_remote_code=True)\n",
        "    \n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    print(\"Cargando configuración del modelo base...\")\n",
        "    try:\n",
        "        config = AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "    except (ValueError, TypeError, KeyError) as e:\n",
        "        if \"rope_scaling\" in str(e):\n",
        "            print(\"WARNING: Error de rope_scaling. Ajustando configuración...\")\n",
        "            try:\n",
        "                import requests\n",
        "                config_path = Path(tempfile.gettempdir()) / \"config_temp.json\"\n",
        "                config_url = f\"https://huggingface.co/{base_model_name}/resolve/main/config.json\"\n",
        "                headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"} if HF_TOKEN else {}\n",
        "                response = requests.get(config_url, headers=headers)\n",
        "                if response.status_code == 200:\n",
        "                    config_path.write_text(response.text)\n",
        "                    config = AutoConfig.from_pretrained(str(config_path.parent), trust_remote_code=True)\n",
        "                    config_path.unlink()\n",
        "                else:\n",
        "                    raise\n",
        "            except Exception:\n",
        "                print(\"WARNING: No se pudo ajustar rope_scaling, continuando...\")\n",
        "                config = AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "        else:\n",
        "            raise\n",
        "    \n",
        "    print(\"Cargando modelo base desde HuggingFace...\")\n",
        "    print(\"NOTA: Esto puede tardar varios minutos (el modelo tiene ~5GB)...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        config=config,\n",
        "        torch_dtype=torch.float32,\n",
        "        device_map=None,\n",
        "        low_cpu_mem_usage=False,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    # Verificar y ajustar tamaño de embeddings si es necesario\n",
        "    print(\"Verificando tamaño de vocabulario...\")\n",
        "    try:\n",
        "        lora_tokenizer_check = AutoTokenizer.from_pretrained(str(lora_path), use_fast=False, trust_remote_code=True)\n",
        "        lora_vocab_size = len(lora_tokenizer_check) if hasattr(lora_tokenizer_check, '__len__') else lora_tokenizer_check.vocab_size\n",
        "        base_vocab_size = base_model.config.vocab_size\n",
        "        \n",
        "        if lora_vocab_size != base_vocab_size:\n",
        "            print(f\"Ajustando tamaño de embeddings: {base_vocab_size} -> {lora_vocab_size}\")\n",
        "            base_model.resize_token_embeddings(lora_vocab_size)\n",
        "            print(f\"Tamaño de embeddings ajustado exitosamente\")\n",
        "        else:\n",
        "            print(f\"Tamaño de vocabulario coincide: {base_vocab_size}\")\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: No se pudo verificar tamaño de vocabulario del LoRA: {e}\")\n",
        "        if hasattr(tokenizer, 'vocab_size') and tokenizer.vocab_size != base_model.config.vocab_size:\n",
        "            print(f\"Ajustando tamaño de embeddings: {base_model.config.vocab_size} -> {tokenizer.vocab_size}\")\n",
        "            base_model.resize_token_embeddings(len(tokenizer))\n",
        "    \n",
        "    # Limpiar adapter_config.json de campos incompatibles\n",
        "    print(\"Verificando y limpiando adapter_config.json...\")\n",
        "    adapter_config_path = lora_path / \"adapter_config.json\"\n",
        "    temp_lora_dir = None\n",
        "    \n",
        "    if adapter_config_path.exists():\n",
        "        adapter_config = json.loads(adapter_config_path.read_text(encoding=\"utf-8\"))\n",
        "        \n",
        "        # Crear una versión limpia del config con solo los campos que PEFT soporta\n",
        "        clean_config = {\n",
        "            \"peft_type\": adapter_config.get(\"peft_type\", \"LORA\"),\n",
        "            \"task_type\": adapter_config.get(\"task_type\", \"CAUSAL_LM\"),\n",
        "            \"base_model_name_or_path\": adapter_config.get(\"base_model_name_or_path\"),\n",
        "            \"r\": adapter_config.get(\"r\"),\n",
        "            \"lora_alpha\": adapter_config.get(\"lora_alpha\"),\n",
        "            \"lora_dropout\": adapter_config.get(\"lora_dropout\", 0.0),\n",
        "            \"bias\": adapter_config.get(\"bias\", \"none\"),\n",
        "            \"target_modules\": adapter_config.get(\"target_modules\"),\n",
        "            \"fan_in_fan_out\": adapter_config.get(\"fan_in_fan_out\", False),\n",
        "            \"inference_mode\": adapter_config.get(\"inference_mode\", True),\n",
        "            \"init_lora_weights\": adapter_config.get(\"init_lora_weights\", True),\n",
        "        }\n",
        "        \n",
        "        # Agregar campos opcionales compatibles\n",
        "        optional_fields = [\"modules_to_save\", \"revision\", \"alpha_pattern\", \"rank_pattern\"]\n",
        "        for field in optional_fields:\n",
        "            if field in adapter_config and adapter_config[field] is not None:\n",
        "                clean_config[field] = adapter_config[field]\n",
        "        \n",
        "        # Verificar si hay diferencias\n",
        "        removed_fields = set(adapter_config.keys()) - set(clean_config.keys())\n",
        "        if removed_fields:\n",
        "            print(f\"WARNING: Removiendo campos incompatibles: {sorted(removed_fields)}\")\n",
        "            # Crear un directorio temporal con el config limpio\n",
        "            temp_lora_dir = Path(tempfile.mkdtemp())\n",
        "            # Copiar todos los archivos excepto adapter_config.json\n",
        "            for file in lora_path.iterdir():\n",
        "                if file.name != \"adapter_config.json\":\n",
        "                    if file.is_file():\n",
        "                        shutil.copy2(file, temp_lora_dir / file.name)\n",
        "                    else:\n",
        "                        shutil.copytree(file, temp_lora_dir / file.name)\n",
        "            # Guardar el config limpio\n",
        "            with open(temp_lora_dir / \"adapter_config.json\", 'w', encoding='utf-8') as f:\n",
        "                json.dump(clean_config, f, indent=2)\n",
        "            \n",
        "            lora_path_to_use = temp_lora_dir\n",
        "            print(f\"Usando configuración temporal limpia\")\n",
        "        else:\n",
        "            lora_path_to_use = lora_path\n",
        "    else:\n",
        "        lora_path_to_use = lora_path\n",
        "    \n",
        "    print(\"Cargando adaptador LoRA...\")\n",
        "    try:\n",
        "        model = PeftModel.from_pretrained(base_model, str(lora_path_to_use), device_map=None)\n",
        "    finally:\n",
        "        # Limpiar directorio temporal si se creó\n",
        "        if temp_lora_dir and temp_lora_dir.exists():\n",
        "            shutil.rmtree(temp_lora_dir, ignore_errors=True)\n",
        "    \n",
        "    print(\"Mergeando LoRA con modelo base...\")\n",
        "    print(\"NOTA: Esto puede tardar varios minutos...\")\n",
        "    model = model.merge_and_unload()\n",
        "    \n",
        "    print(f\"Guardando modelo mergeado en {output_dir}...\")\n",
        "    print(\"NOTA: Esto puede tardar varios minutos...\")\n",
        "    model.save_pretrained(str(output_path), safe_serialization=True)\n",
        "    tokenizer.save_pretrained(str(output_path))\n",
        "    \n",
        "    print(f\"Modelo mergeado guardado exitosamente en {output_dir}\")\n",
        "    return output_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_to_s3(local_path: Path, s3_bucket: str, s3_key_prefix: str, region: str = \"us-east-1\"):\n",
        "    \"\"\"Sube el modelo mergeado a S3\"\"\"\n",
        "    if not S3_AVAILABLE:\n",
        "        print(\"ERROR: boto3 no está disponible. No se puede subir a S3.\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"\\nSubiendo modelo a S3...\")\n",
        "    print(f\"  Bucket: {s3_bucket}\")\n",
        "    print(f\"  Key prefix: {s3_key_prefix}\")\n",
        "    \n",
        "    s3_client = boto3.client('s3', region_name=region)\n",
        "    \n",
        "    try:\n",
        "        total_files = sum(1 for _ in local_path.rglob('*') if _.is_file())\n",
        "        uploaded = 0\n",
        "        \n",
        "        for file_path in local_path.rglob('*'):\n",
        "            if file_path.is_file():\n",
        "                relative_path = file_path.relative_to(local_path)\n",
        "                s3_key = f\"{s3_key_prefix}/{relative_path}\".replace(\"\\\\\", \"/\")\n",
        "                \n",
        "                print(f\"  Subiendo {relative_path}... ({uploaded + 1}/{total_files})\")\n",
        "                s3_client.upload_file(str(file_path), s3_bucket, s3_key)\n",
        "                uploaded += 1\n",
        "        \n",
        "        print(f\"\\nModelo subido exitosamente a s3://{s3_bucket}/{s3_key_prefix}/\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR al subir a S3: {e}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejecutar Merge y Subida\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PASO 1: Mergeando LoRA con modelo base\n",
            "============================================================\n",
            "Mergeando LoRA desde generacion/ollama/outputs/meta-llama__Llama-3.2-3B-Instruct-6_epocas/final...\n",
            "Autenticándose con Hugging Face...\n",
            "Autenticación exitosa con Hugging Face\n",
            "Path relativo detectado, resolviendo desde raíz del proyecto: d:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\generacion\\ollama\\outputs\\meta-llama__Llama-3.2-3B-Instruct-6_epocas\\final\n",
            "Modelo base: meta-llama/Llama-3.2-3B-Instruct\n",
            "Directorio de salida: generacion\\ollama\\temp_merged_model\n",
            "Cargando tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Julian Rico\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-3B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando configuración del modelo base...\n",
            "Cargando modelo base desde HuggingFace...\n",
            "NOTA: Esto puede tardar varios minutos (el modelo tiene ~5GB)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Downloading shards:  50%|█████     | 1/2 [01:26<01:26, 86.45s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Downloading shards: 100%|██████████| 2/2 [01:52<00:00, 56.28s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verificando tamaño de vocabulario...\n",
            "Ajustando tamaño de embeddings: 128256 -> 128257\n",
            "Tamaño de embeddings ajustado exitosamente\n",
            "Verificando y limpiando adapter_config.json...\n",
            "WARNING: Removiendo campos incompatibles: ['auto_mapping', 'corda_config', 'eva_config', 'exclude_modules', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_bias', 'megatron_config', 'megatron_core', 'modules_to_save', 'qalora_group_size', 'revision', 'target_parameters', 'trainable_token_indices', 'use_dora', 'use_qalora', 'use_rslora']\n",
            "Usando configuración temporal limpia\n",
            "Cargando adaptador LoRA...\n",
            "Mergeando LoRA con modelo base...\n",
            "NOTA: Esto puede tardar varios minutos...\n",
            "Guardando modelo mergeado en generacion\\ollama\\temp_merged_model...\n",
            "NOTA: Esto puede tardar varios minutos...\n",
            "Modelo mergeado guardado exitosamente en generacion\\ollama\\temp_merged_model\n",
            "\n",
            "============================================================\n",
            "PASO 2: Subiendo modelo mergeado a S3\n",
            "============================================================\n",
            "\n",
            "Subiendo modelo a S3...\n",
            "  Bucket: modelo-generador-maia-g8\n",
            "  Key prefix: merged-models/meta-llama__Llama-3.2-3B-Instruct-6_epocas\n",
            "  Subiendo config.json... (1/9)\n",
            "  Subiendo generation_config.json... (2/9)\n",
            "  Subiendo model-00001-of-00003.safetensors... (3/9)\n",
            "  Subiendo model-00002-of-00003.safetensors... (4/9)\n",
            "  Subiendo model-00003-of-00003.safetensors... (5/9)\n",
            "  Subiendo model.safetensors.index.json... (6/9)\n",
            "  Subiendo special_tokens_map.json... (7/9)\n",
            "  Subiendo tokenizer.json... (8/9)\n",
            "  Subiendo tokenizer_config.json... (9/9)\n",
            "\n",
            "Modelo subido exitosamente a s3://modelo-generador-maia-g8/merged-models/meta-llama__Llama-3.2-3B-Instruct-6_epocas/\n",
            "\n",
            "============================================================\n",
            "PROCESO COMPLETADO EXITOSAMENTE\n",
            "============================================================\n",
            "Modelo mergeado disponible en: s3://modelo-generador-maia-g8/merged-models/meta-llama__Llama-3.2-3B-Instruct-6_epocas/\n",
            "\n",
            "Ahora puedes hacer el build con:\n",
            "  make build-generador-image MODEL_NAME=meta-llama__Llama-3.2-3B-Instruct-6_epocas\n",
            "\n",
            "Limpiando archivos temporales...\n",
            "Limpieza completada\n"
          ]
        }
      ],
      "source": [
        "# Crear directorio temporal para el modelo mergeado\n",
        "# Usar un directorio en el proyecto en lugar del temp de Windows\n",
        "temp_output = Path(\"generacion/ollama/temp_merged_model\")\n",
        "temp_output.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"PASO 1: Mergeando LoRA con modelo base\")\n",
        "    print(\"=\" * 60)\n",
        "    output_path = merge_lora_with_base(LORA_PATH, str(temp_output))\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PASO 2: Subiendo modelo mergeado a S3\")\n",
        "    print(\"=\" * 60)\n",
        "    s3_key_prefix = f\"merged-models/{MODEL_NAME}\"\n",
        "    success = upload_to_s3(output_path, MODEL_S3_BUCKET, s3_key_prefix, AWS_REGION)\n",
        "    \n",
        "    if success:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"PROCESO COMPLETADO EXITOSAMENTE\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Modelo mergeado disponible en: s3://{MODEL_S3_BUCKET}/{s3_key_prefix}/\")\n",
        "        print(f\"\\nAhora puedes hacer el build con:\")\n",
        "        print(f\"  make build-generador-image MODEL_NAME={MODEL_NAME}\")\n",
        "    else:\n",
        "        print(\"\\nERROR: No se pudo subir el modelo a S3\")\n",
        "        \n",
        "finally:\n",
        "    print(f\"\\nLimpiando archivos temporales...\")\n",
        "    shutil.rmtree(temp_output, ignore_errors=True)\n",
        "    print(\"Limpieza completada\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_generacion",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
