{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ada16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | GPU: NVIDIA GeForce RTX 3090\n",
      "Model: meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, time, json, random, csv, gc, re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import sys; from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parents[1])) \n",
    "import KEYS\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    ")\n",
    "from transformers.trainer_callback import TrainerCallback, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURACIÓN\n",
    "# ----------------------------\n",
    "MODEL_ID = os.getenv(\"MODEL_ID\", \"meta-llama/Llama-3.2-3B-Instruct\")   \n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)                                  \n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CUT_OFF_LEN     = int(os.getenv(\"CUT_OFF_LEN\", \"1024\"))    # longitud máxima del texto de entrada (tokens)\n",
    "MAX_NEW_TOKENS  = int(os.getenv(\"MAX_NEW_TOKENS\", \"512\"))  # límite de tokens que puede generar el modelo\n",
    "TEMPERATURE     = float(os.getenv(\"TEMPERATURE\", \"0.2\"))   # controla la aleatoriedad (más bajo = más determinista)\n",
    "TOP_P           = float(os.getenv(\"TOP_P\", \"0.95\"))        # limita la generación a los tokens más probables (≈TOP_P % de probabilidad acumulada)\n",
    "N_TRAIN         = int(os.getenv(\"N_TRAIN\", \"20\"))        # número de muestras de entrenamiento (subconjunto)\n",
    "EVAL_STEPS      = int(os.getenv(\"EVAL_STEPS\", \"10\"))       # cada cuántos pasos se evalúa el modelo\n",
    "LOG_STEPS       = int(os.getenv(\"LOG_STEPS\", \"10\"))        # cada cuántos pasos se registran métricas (logs)\n",
    "SAVE_STEPS      = int(os.getenv(\"SAVE_STEPS\", \"50\"))       # cada cuántos pasos se guarda un checkpoint\n",
    "NUM_EPOCHS      = int(os.getenv(\"NUM_EPOCHS\", \"3\"))        # número total de épocas de entrenamiento\n",
    "BATCH_TRAIN     = int(os.getenv(\"BATCH_TRAIN\", \"4\"))       # tamaño de lote (batch) para entrenamiento\n",
    "BATCH_EVAL      = int(os.getenv(\"BATCH_EVAL\", \"4\"))        # tamaño de lote (batch) para validación\n",
    "GRAD_ACC_STEPS  = int(os.getenv(\"GRAD_ACC_STEPS\", \"16\"))   # pasos para acumular gradientes (simula batch grande)\n",
    "LR              = float(os.getenv(\"LR\", \"1e-4\"))           # tasa de aprendizaje\n",
    "WARMUP_RATIO    = float(os.getenv(\"WARMUP_RATIO\", \"0.05\")) # fracción inicial del entrenamiento usada para warmup\n",
    "SEED            = int(os.getenv(\"SEED\", \"99\"))             # semilla aleatoria para reproducibilidad\n",
    "\n",
    "# LoRA \n",
    "LORA_R          = int(os.getenv(\"LORA_R\", \"16\"))           # rango de la descomposición de matrices\n",
    "LORA_ALPHA      = int(os.getenv(\"LORA_ALPHA\", \"32\"))       # escala del aprendizaje en capas LoRA\n",
    "LORA_DROPOUT    = float(os.getenv(\"LORA_DROPOUT\", \"0.05\")) # probabilidad de dropout en LoRA\n",
    "\n",
    "# Sliding Window\n",
    "USE_SLIDING_WINDOW = os.getenv(\"USE_SLIDING_WINDOW\", \"1\") == \"1\"  # 1=on, 0=off\n",
    "SW_WINDOW_TOKS    = int(os.getenv(\"SW_WINDOW_TOKS\", \"512\"))       # tamaño de ventana en tokens\n",
    "SW_OVERLAP_TOKS   = int(os.getenv(\"SW_OVERLAP_TOKS\", \"128\"))      # solapamiento en tokens\n",
    "SW_FUSION_MODE    = os.getenv(\"SW_FUSION_MODE\", \"concat\")         # por ahora: 'concat'\n",
    "\n",
    "# In Context Learning (One-shot/Few-shot) \n",
    "USE_FEWSHOT    = os.getenv(\"USE_FEWSHOT\", \"0\") == \"1\"      # Flag para indicar si se usa ICL o no\n",
    "FEWSHOT_K      = int(os.getenv(\"FEWSHOT_K\", \"2\"))          # Cantidad de ejemplos a utilizar\n",
    "FEWSHOT_SEED   = int(os.getenv(\"FEWSHOT_SEED\", str(SEED))) # semilla aleatoria para reproducibilidad\n",
    "if USE_FEWSHOT:                                            # 'zero-shot' | 'one-shot' | 'few-shot-2'\n",
    "    ICL_MODE = f\"few-shot:{max(1, FEWSHOT_K)}\"\n",
    "             \n",
    "# Métricas\n",
    "LOSS_WEIGHTS    = [0.2, 0.25, 0.4, 0.0, 0.1]               # Peso ponderado de las métricas en el entrenamiento\n",
    "\n",
    "# Salidas\n",
    "SAFE_MODEL_NAME = MODEL_ID.replace(\"/\", \"__\")\n",
    "OUTPUT_DIR   = os.getenv(\"OUTPUT_DIR\", f\"outputs/{SAFE_MODEL_NAME}-focus-FKGD9\")\n",
    "METRICS_CSV  = f\"{OUTPUT_DIR}/train_val_metrics.csv\"\n",
    "PLOT_PATH    = f\"{OUTPUT_DIR}/loss_curves.png\"\n",
    "FINAL_DIR    = f\"{OUTPUT_DIR}/final\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_DIR, exist_ok=True)\n",
    "\n",
    "# Otros\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "print(\"Device:\", DEVICE, \"| GPU:\", torch.cuda.get_device_name(0) if DEVICE==\"cuda\" else \"CPU\")\n",
    "print(\"Model:\", MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc838e5",
   "metadata": {},
   "source": [
    "## Configurar el wrapper de métricas y funcion de pérdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb516b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca la carpeta 'metricas' hacia arriba y añade a sys.path\n",
    "HERE = Path.cwd()\n",
    "found = None\n",
    "for p in [HERE] + list(HERE.parents):\n",
    "    if (p / \"metricas\").is_dir():\n",
    "        found = p\n",
    "        break\n",
    "if not found:\n",
    "    raise RuntimeError(\"No se encontró la carpeta 'metricas' en ningún ancestro.\")\n",
    "if str(found) not in sys.path:\n",
    "    sys.path.insert(0, str(found))\n",
    "\n",
    "from metricas.metrics_client import getLoss  # usa http://localhost:8000\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_composite_loss(model, tokenizer, val_df, sample_size=32):\n",
    "    if len(val_df) == 0:\n",
    "        return None\n",
    "    sub = val_df.sample(n=min(sample_size, len(val_df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    outs = []\n",
    "    sources = sub[\"source_text\"].tolist()\n",
    "    for s in sources:\n",
    "        if USE_SLIDING_WINDOW:\n",
    "            parts = _token_windows_from_text(s, tokenizer, SW_WINDOW_TOKS, SW_OVERLAP_TOKS)\n",
    "            gens = []\n",
    "            for w in parts:\n",
    "                prompt = tokenizer.apply_chat_template(\n",
    "                    [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\",   \"content\": USER_PREFIX + w}],\n",
    "                    tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=CUT_OFF_LEN).to(DEVICE)\n",
    "                gen = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max(64, min(MAX_NEW_TOKENS, SW_WINDOW_TOKS)),  # por ventana\n",
    "                    do_sample=True,\n",
    "                    temperature=TEMPERATURE,\n",
    "                    top_p=TOP_P,\n",
    "                    eos_token_id=STOP_EOS_IDS,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                )\n",
    "                text = tokenizer.decode(gen[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "                gens.append(text)\n",
    "            fused = _fuse_generations(gens, mode=SW_FUSION_MODE)\n",
    "            outs.append(fused)\n",
    "        else:\n",
    "            prompt@torch.no_grad()\n",
    "def eval_composite_loss(model, tokenizer, val_df, sample_size=64):\n",
    "    if len(val_df) == 0:\n",
    "        return None\n",
    "    sub = val_df.sample(n=min(sample_size, len(val_df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    fused_outs = []\n",
    "\n",
    "    for _, row in sub.iterrows():\n",
    "        src = str(row[\"source_text\"])\n",
    "\n",
    "        # Ventanas del source\n",
    "        if USE_SLIDING_WINDOW:\n",
    "            src_windows_ids = make_windows(src, tokenizer, SW_WINDOW_TOKS, SW_OVERLAP_TOKS)\n",
    "            src_segments = [decode_ids(w, tokenizer) for w in src_windows_ids]\n",
    "        else:\n",
    "            src_segments = [src]\n",
    "\n",
    "        seg_outputs = []\n",
    "        for seg in src_segments:\n",
    "            chat = build_chat(seg, tgt_text=None, icl_mode=ICL_MODE, icl_examples=[])\n",
    "            prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=CUT_OFF_LEN).to(DEVICE)\n",
    "\n",
    "            eos_ids = [tokenizer.eos_token_id] + ([EOS_ID] if EOS_ID is not None else [])\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=min(MAX_NEW_TOKENS, 512),\n",
    "                do_sample=False,\n",
    "                no_repeat_ngram_size=3,\n",
    "                repetition_penalty=1.05,\n",
    "                eos_token_id=eos_ids,\n",
    "                pad_token_id=tokenizer.pad_token_id,)\n",
    "\n",
    "            out = tokenizer.batch_decode(gen[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)[0]\n",
    "            seg_outputs.append(out)\n",
    "\n",
    "        fused = fuse_segments(seg_outputs, SW_FUSION_MODE)\n",
    "        fused_outs.append(fused)\n",
    "\n",
    "    losses = getLoss(\n",
    "        sub[\"source_text\"].tolist(),\n",
    "        sub[\"target_text\"].tolist(),\n",
    "        fused_outs,\n",
    "        weights=LOSS_WEIGHTS\n",
    "    )\n",
    "    if isinstance(losses, list):\n",
    "        return float(sum(losses)/len(losses))\n",
    "    return float(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb01dd",
   "metadata": {},
   "source": [
    "## Carga y repartición de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed168056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20 pares \n",
      "Val:   218 pares\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = os.getenv(\"CSV_PATH\", \"../../data/pls_abstract_pairs_with_metrics.csv\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Split en val/test\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "if N_TRAIN:\n",
    "    train_df = train_df.sample(n=min(N_TRAIN, len(train_df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df)} pares \\nVal:   {len(val_df)} pares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574690c",
   "metadata": {},
   "source": [
    "## Definir Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d000783",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ICL_EXAMPLES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 47\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Espacio para más estrategias (e.g., RLAIF/rerank/resumen adicional)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([g\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m gens \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mand\u001b[39;00m g\u001b[38;5;241m.\u001b[39mstrip()])\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_chat\u001b[39m(\n\u001b[0;32m     43\u001b[0m     src_text: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     44\u001b[0m     tgt_text: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m     46\u001b[0m     icl_mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m ICL_MODE,       \u001b[38;5;66;03m# 'zero-shot' | 'one-shot' | 'few-shot:k'\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     icl_examples: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mICL_EXAMPLES\u001b[49m,  \u001b[38;5;66;03m# [(src_ex, tgt_ex), ...]\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     user_prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m USER_PREFIX,\n\u001b[0;32m     49\u001b[0m     system_prompt: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m SYSTEM_PROMPT,\n\u001b[0;32m     50\u001b[0m     eos_token: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|sentence_end|>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     51\u001b[0m ):\n\u001b[0;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    Construye el chat list para training o inferencia:\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m      - Training: pasar tgt_text (se añade EOS si falta).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m          'few-shot:k'  -> usa k ejemplos (primeros k de icl_examples)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     msgs \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt}]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ICL_EXAMPLES' is not defined"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You simplify clinical trial protocol text into a plain-language summary for the general public. \"\n",
    "    \"Keep to 6–8th grade readability, avoid diagnoses and speculation, no hallucinations, \"\n",
    "    \"and preserve key facts (objective, population, interventions, outcomes, timelines, safety).\"\n",
    ")\n",
    "USER_PREFIX = \"Using the following clinical trial protocol text as input, create a plain language summary.\\n\\n\"\n",
    "\n",
    "def _token_windows_from_text(text: str, tokenizer, window_toks: int, overlap_toks: int):\n",
    "    \"\"\"Corta el texto en ventanas solapadas *por tokens*. Devuelve lista de strings.\"\"\"\n",
    "    ids = tokenizer(text, add_special_tokens=False, return_attention_mask=False)[\"input_ids\"]\n",
    "    if not ids:\n",
    "        return [text]\n",
    "    stride = max(1, window_toks - overlap_toks)\n",
    "    chunks = []\n",
    "    for start in range(0, len(ids), stride):\n",
    "        end = start + window_toks\n",
    "        sub_ids = ids[start:end]\n",
    "        if not sub_ids:\n",
    "            break\n",
    "        chunks.append(tokenizer.decode(sub_ids, skip_special_tokens=True))\n",
    "        if end >= len(ids):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def _fuse_generations(gens: list[str], mode: str = \"concat\") -> str:\n",
    "    \"\"\"Fusiona salidas por ventana. Modo simple: concat con normalización ligera.\"\"\"\n",
    "    if not gens:\n",
    "        return \"\"\n",
    "    if mode == \"concat\":\n",
    "        # Limpieza mínima + unión con saltos\n",
    "        cleaned = [g.strip() for g in gens if g and g.strip()]\n",
    "        # Evita duplicados exactos consecutivos\n",
    "        fused = []\n",
    "        for g in cleaned:\n",
    "            if not fused or g != fused[-1]:\n",
    "                fused.append(g)\n",
    "        return \"\\n\".join(fused).strip()\n",
    "    # Espacio para más estrategias (e.g., RLAIF/rerank/resumen adicional)\n",
    "    return \" \".join([g.strip() for g in gens if g and g.strip()]).strip()\n",
    "\n",
    "\n",
    "def build_chat(\n",
    "    src_text: str,\n",
    "    tgt_text: str | None,\n",
    "    *,\n",
    "    icl_mode: str = ICL_MODE,                 # 'zero-shot' | 'one-shot' | 'few-shot:k'\n",
    "    icl_examples: list[tuple[str, str]] = [], # [(src_ex, tgt_ex), ...] si los usas\n",
    "    user_prefix: str = USER_PREFIX,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    eos_token: str = \"<|sentence_end|>\",\n",
    "):\n",
    "    \"\"\"\n",
    "    - Training: pasar tgt_text (se añade EOS si falta).\n",
    "    - Inferencia: tgt_text=None (no se agrega assistant).\n",
    "    - ICL:\n",
    "        'zero-shot' → sin ejemplos\n",
    "        'one-shot'  → 1 ejemplo\n",
    "        'few-shot:k'→ k ejemplos (primeros k)\n",
    "    \"\"\"\n",
    "    msgs = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    mode = (icl_mode or \"zero-shot\").strip().lower()\n",
    "    k = 0\n",
    "    if mode == \"one-shot\":\n",
    "        k = 1\n",
    "    elif mode.startswith(\"few-shot\"):\n",
    "        parts = mode.split(\":\")\n",
    "        k = int(parts[1]) if len(parts) == 2 and parts[1].isdigit() else 2\n",
    "\n",
    "    if k > 0 and icl_examples:\n",
    "        for ex_src, ex_tgt in icl_examples[:k]:\n",
    "            msgs.append({\"role\": \"user\", \"content\": user_prefix + str(ex_src)})\n",
    "            ex_tgt_clean = str(ex_tgt).rstrip()\n",
    "            if not ex_tgt_clean.endswith(eos_token):\n",
    "                ex_tgt_clean += f\" {eos_token}\"\n",
    "            msgs.append({\"role\": \"assistant\", \"content\": ex_tgt_clean})\n",
    "\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_prefix + str(src_text)})\n",
    "\n",
    "    if tgt_text is not None:\n",
    "        tgt_clean = str(tgt_text).rstrip()\n",
    "        if not tgt_clean.endswith(eos_token):\n",
    "            tgt_clean += f\" {eos_token}\"\n",
    "        msgs.append({\"role\": \"assistant\", \"content\": tgt_clean})\n",
    "\n",
    "    return msgs\n",
    "\n",
    "\n",
    "def make_windows(text: str, tokenizer, max_window_toks: int, overlap_toks: int) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Devuelve lista de ventanas en IDs (no strings), con solapamiento controlado.\n",
    "    No corta dentro de los prompts; solo tokeniza el texto plano.\n",
    "    \"\"\"\n",
    "    ids = tokenizer(str(text), add_special_tokens=False)[\"input_ids\"]\n",
    "    if not ids:\n",
    "        return [[]]\n",
    "\n",
    "    win = max(8, int(max_window_toks))\n",
    "    ovl = max(0, int(overlap_toks))\n",
    "    step = max(1, win - ovl)\n",
    "\n",
    "    windows = []\n",
    "    i = 0\n",
    "    n = len(ids)\n",
    "    while i < n:\n",
    "        j = min(i + win, n)\n",
    "        windows.append(ids[i:j])\n",
    "        if j == n:\n",
    "            break\n",
    "        i += step\n",
    "    return windows\n",
    "\n",
    "def decode_ids(ids: list[int], tokenizer) -> str:\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def fuse_segments(segments: list[str], mode: str = SW_FUSION_MODE) -> str:\n",
    "    \"\"\"\n",
    "    Fusión simple:\n",
    "      - 'concat' (default): concatena con espacio.\n",
    "      (Si luego quieres 'abstractive' o 'vote', lo extiendes aquí.)\n",
    "    \"\"\"\n",
    "    segs = [s.strip() for s in segments if s and s.strip()]\n",
    "    return \" \".join(segs).strip()\n",
    "\n",
    "\n",
    "\n",
    "def encode_supervised(batch, tokenizer):\n",
    "    out = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    max_len = CUT_OFF_LEN\n",
    "    eos_id = tokenizer.convert_tokens_to_ids(\"<|sentence_end|>\")\n",
    "\n",
    "    for s, t in zip(batch[\"source_text\"], batch[\"target_text\"]):\n",
    "        src_full = str(s)\n",
    "        tgt_full = str(t)\n",
    "\n",
    "        # Sliding-window ON → iterar ventanas; OFF → una sola \"ventana\" con todo\n",
    "        if USE_SLIDING_WINDOW:\n",
    "            src_windows_ids = make_windows(src_full, tokenizer, SW_WINDOW_TOKS, SW_OVERLAP_TOKS)\n",
    "            src_segments = [decode_ids(w, tokenizer) for w in src_windows_ids]\n",
    "        else:\n",
    "            src_segments = [src_full]\n",
    "\n",
    "        # Para training generamos un ejemplo por segmento (supervisión densa)\n",
    "        for src_seg in src_segments:\n",
    "            # Hasta 5 intentos de recorte del *source* si el EOS del target no entra\n",
    "            src_try = src_seg\n",
    "            for _ in range(5):\n",
    "                chat = build_chat(src_try, tgt_full, icl_mode=ICL_MODE, icl_examples=[])\n",
    "                text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "                toks = tokenizer(text, truncation=True, max_length=max_len, padding=False)\n",
    "                if eos_id in toks[\"input_ids\"]:\n",
    "                    out[\"input_ids\"].append(toks[\"input_ids\"])\n",
    "                    out[\"attention_mask\"].append(toks[\"attention_mask\"])\n",
    "                    break\n",
    "\n",
    "                # recorte conservador del source si no entra el EOS del target\n",
    "                if len(src_try) < 80:\n",
    "                    out[\"input_ids\"].append(toks[\"input_ids\"])\n",
    "                    out[\"attention_mask\"].append(toks[\"attention_mask\"])\n",
    "                    break\n",
    "                src_try = src_try[: int(len(src_try) * 0.85)]\n",
    "            else:\n",
    "                # caso anómalo: registrar igual\n",
    "                out[\"input_ids\"].append(toks[\"input_ids\"])\n",
    "                out[\"attention_mask\"].append(toks[\"attention_mask\"])\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_id: str, hf_token: str | None = HF_TOKEN):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True, token=hf_token, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"left\"\n",
    "    return tok\n",
    "\n",
    "def load_causallm(model_id: str, device: str = DEVICE, hf_token: str | None = HF_TOKEN):\n",
    "    # Verifica que sea un modelo de lenguaje causal compatible (no VL)\n",
    "    try:\n",
    "        cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=True, token=hf_token)\n",
    "        arch_ok = any(\"CausalLM\" in a for a in getattr(cfg, \"architectures\", []) or [])\n",
    "    except Exception:\n",
    "        arch_ok = True  # algunos repos no exponen architectures; intentamos cargar de todas formas\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16 if device.startswith(\"cuda\") else torch.float32,\n",
    "        trust_remote_code=True, token=hf_token\n",
    "    ).to(device)\n",
    "\n",
    "    # Si no es CausalLM, esto normalmente falla al forward; advertimos:\n",
    "    if not arch_ok:\n",
    "        print(\"[WARN] El repo no declara arquitectura CausalLM. Si falla el forward, usar un modelo TEXT (no VL).\")\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    return model\n",
    "\n",
    "tokenizer = load_tokenizer(MODEL_ID)\n",
    "model     = load_causallm(MODEL_ID)\n",
    "special = {\"additional_special_tokens\": [\"<|sentence_end|>\"]}\n",
    "added = tokenizer.add_special_tokens(special)\n",
    "if added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tokens de parada y fin de oracion\n",
    "EOS_ID = tokenizer.convert_tokens_to_ids(\"<|sentence_end|>\")\n",
    "STOP_EOS_IDS = list({tokenizer.eos_token_id, EOS_ID})\n",
    "print(\"Tokenizer & model listos. Device:\", next(model.parameters()).device)\n",
    "print(f\"[CONF] SlidingWindow={'ON' if USE_SLIDING_WINDOW else 'OFF'} \"\n",
    "      f\"(window={SW_WINDOW_TOKS}, overlap={SW_OVERLAP_TOKS}, fusion={SW_FUSION_MODE}); \"\n",
    "      f\"ICL_MODE={ICL_MODE}\")\n",
    "\n",
    "\n",
    "# conjunto para few-shots desde training\n",
    "FEWSHOT_EXAMPLES = []\n",
    "if USE_FEWSHOT and len(train_df) > 0 and FEWSHOT_K > 0:\n",
    "    rng = np.random.RandomState(FEWSHOT_SEED)\n",
    "    idx = rng.choice(len(train_df), size=min(FEWSHOT_K, len(train_df)), replace=False)\n",
    "    FEWSHOT_EXAMPLES = [(train_df.iloc[i][\"source_text\"], train_df.iloc[i][\"target_text\"]) for i in idx]\n",
    "\n",
    "k = len(FEWSHOT_EXAMPLES) if (USE_FEWSHOT and FEWSHOT_EXAMPLES) else 0\n",
    "ICL_MODE = \"zero-shot\" if k == 0 else (\"one-shot\" if k == 1 else \"few-shot\")\n",
    "print(f\"[ICL] mode={ICL_MODE}\" + (f\" | k={k}\" if k else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22356315",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = Dataset.from_pandas(train_df[[\"source_text\",\"target_text\"]]).map(\n",
    "    lambda b: encode_supervised(b, tokenizer), batched=True, remove_columns=[\"source_text\",\"target_text\"]\n",
    ")\n",
    "hf_val = Dataset.from_pandas(val_df[[\"source_text\",\"target_text\"]]).map(\n",
    "    lambda b: encode_supervised(b, tokenizer), batched=True, remove_columns=[\"source_text\",\"target_text\"]\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "print(hf_train, hf_val)\n",
    "\n",
    "print(f\"[sliding-window] {'ON' if USE_SLIDING_WINDOW else 'OFF'} | window={SW_WINDOW_TOKS}, overlap={SW_OVERLAP_TOKS}, fusion='{SW_FUSION_MODE}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d797453",
   "metadata": {},
   "source": [
    "## LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a691d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_find_lora_targets(model, extra_patterns=None):\n",
    "    common = {\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "        \"wi\",\"wo\",\"wq\",\"wk\",\"wv\",\"W_pack\",\"query_key_value\"\n",
    "    }\n",
    "    if extra_patterns:\n",
    "        common |= set(extra_patterns)\n",
    "    found = set()\n",
    "    for name, module in model.named_modules():\n",
    "        base = name.split(\".\")[-1]\n",
    "        if base in common:\n",
    "            found.add(base)\n",
    "    if not found:\n",
    "        found = {\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"}\n",
    "    return sorted(found)\n",
    "\n",
    "targets = auto_find_lora_targets(model)\n",
    "print(\"LoRA targets:\", targets)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=targets\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.train()\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable/1e6:.2f}M / {total/1e6:.2f}M ({100*trainable/total:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c185436",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39149ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValCSVLogger(TrainerCallback):\n",
    "    def __init__(self, csv_path=METRICS_CSV):\n",
    "        self.csv_path = csv_path\n",
    "        os.makedirs(os.path.dirname(self.csv_path), exist_ok=True)\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
    "                csv.writer(f).writerow([\"step\",\"train_loss\",\"eval_loss\",\"lr\",\"timestamp\",\"composite_loss\"])\n",
    "\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        logs = kwargs.get(\"logs\", {})\n",
    "        step = state.global_step\n",
    "        tl   = logs.get(\"loss\")\n",
    "        el   = logs.get(\"eval_loss\")\n",
    "        lr   = logs.get(\"learning_rate\")\n",
    "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([step, tl, el, lr, time.time(), None])\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        comp = eval_composite_loss(kwargs[\"model\"], tokenizer, val_df)\n",
    "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([state.global_step, None, kwargs[\"metrics\"].get(\"eval_loss\"), None, time.time(), comp])\n",
    "\n",
    "early_stop = EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=1e-4)\n",
    "csv_logger = TrainValCSVLogger(METRICS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8bef3",
   "metadata": {},
   "source": [
    "## PEFT (Ajuste Fino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ec879",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    fp16=(DEVICE==\"cuda\"),\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.add_callback(csv_logger)\n",
    "trainer.add_callback(early_stop)\n",
    "\n",
    "approx_steps = math.ceil(len(hf_train) / max(1, BATCH_TRAIN) / max(1, GRAD_ACC_STEPS)) * NUM_EPOCHS\n",
    "print(\"Aprox. training steps:\", approx_steps)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90486d4",
   "metadata": {},
   "source": [
    "## Guardado y demo de inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b337f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado final (PEFT adapters + tokenizer)\n",
    "trainer.model.save_pretrained(FINAL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_DIR)\n",
    "print(\"Guardado en:\", FINAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo de inferencia corta y estable (greedy + cierre correcto)\n",
    "demo_src = val_df.iloc[0][\"source_text\"]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    build_chat(demo_src, tgt=\"\", fewshots=FEWSHOT_EXAMPLES if (USE_FEWSHOT and FEWSHOT_EXAMPLES) else None)[:-1],\n",
    "    tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Texto original:\\n\", demo_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670712cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "eos_ids = [tokenizer.eos_token_id] + ([EOS_ID] if EOS_ID is not None else [])\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,           \n",
    "        do_sample=False,              \n",
    "        no_repeat_ngram_size=0,       \n",
    "        repetition_penalty=1.05,      \n",
    "        eos_token_id=STOP_EOS_IDS,        \n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "decoded = tokenizer.decode(gen[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(\"Texto generado:\\n\")\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f0752",
   "metadata": {},
   "source": [
    "## Gráfica de rendimiento (entrenamiento v.s. validación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.read_csv(METRICS_CSV)\n",
    "for c in (\"step\",\"train_loss\",\"eval_loss\",\"lr\"):\n",
    "    if c in dfm.columns: dfm[c] = pd.to_numeric(dfm[c], errors=\"coerce\")\n",
    "dfm = dfm.dropna(subset=[\"train_loss\",\"eval_loss\",\"lr\"], how=\"all\")\n",
    "dfm = dfm.sort_values(\"step\").groupby(\"step\", as_index=False).last()\n",
    "\n",
    "xs_tr = dfm[\"step\"].to_numpy()\n",
    "ys_tr = dfm[\"train_loss\"].to_numpy()\n",
    "xs_ev = dfm[\"step\"].to_numpy()\n",
    "ys_ev = dfm[\"eval_loss\"].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "if np.isfinite(ys_tr).any(): plt.plot(xs_tr, ys_tr, \"-o\", label=\"train_loss\", lw=2, ms=4)\n",
    "if np.isfinite(ys_ev).any(): plt.plot(xs_ev, ys_ev, \"-o\", label=\"eval_loss\",  lw=2, ms=4)\n",
    "\n",
    "vals = np.concatenate([a[~np.isnan(a)] for a in [ys_tr, ys_ev] if len(a)])\n",
    "if len(vals):\n",
    "    pad = max(1e-4, 0.08*(vals.max()-vals.min()))\n",
    "    plt.ylim(vals.min()-pad, vals.max()+pad)\n",
    "\n",
    "plt.title(f\"Training vs Validation Loss — {SAFE_MODEL_NAME}\")\n",
    "plt.xlabel(\"step\"); plt.ylabel(\"loss\"); plt.title(\"Training vs Validation Loss\")\n",
    "plt.grid(alpha=0.3); plt.legend()\n",
    "plt.savefig(PLOT_PATH, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot guardado en:\", PLOT_PATH)\n",
    "print(\"Métricas CSV:\", METRICS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2c295",
   "metadata": {},
   "source": [
    "## Experimento (mlflow -> Databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c22411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "from mlflow.data import from_pandas\n",
    "\n",
    "# Configuración MLflow\n",
    "os.environ[\"DATABRICKS_HOST\"] = KEYS.DATABRICKS_HOST\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = KEYS.DATABRICKS_TOKEN\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(f\"/GeneracionDeResumenes/Resultados_Modelos\")\n",
    "\n",
    "mlflow.end_run()\n",
    "run_name = f\"{SAFE_MODEL_NAME}-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    ds_train = from_pandas(train_df, source=\"Cochrane\", name=\"Cochrane_v1\")\n",
    "    mlflow.log_input(ds_train, context=\"training\")\n",
    "    mlflow.log_params({\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"PEFT\":\"LoRa\",\n",
    "        \"sliding_window\": USE_SLIDING_WINDOW,\n",
    "        \"sw_window_toks\": SW_WINDOW_TOKS,\n",
    "        \"sw_overlap_toks\": SW_OVERLAP_TOKS,\n",
    "        \"sw_fusion_mode\": SW_FUSION_MODE,\n",
    "        \"icl_mode\": ICL_MODE,\n",
    "        \"fp16\": (DEVICE==\"cuda\"),\n",
    "        \"cut_off_len\": CUT_OFF_LEN,\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_train\": BATCH_TRAIN,\n",
    "        \"batch_eval\": BATCH_EVAL,\n",
    "        \"grad_acc_steps\": GRAD_ACC_STEPS,\n",
    "        \"lr\": LR,\n",
    "        \"lora_r\": LORA_R,\n",
    "        \"lora_alpha\": LORA_ALPHA,\n",
    "        \"lora_dropout\": LORA_DROPOUT,\n",
    "        \"loss_weights\": json.dumps(LOSS_WEIGHTS),\n",
    "        \"n_train_rows\": len(train_df),\n",
    "        \"n_val_rows\": len(val_df),\n",
    "    })      \n",
    "\n",
    "    # Métricas finales (si existen en CSV)\n",
    "    try:\n",
    "        df_metrics = pd.read_csv(METRICS_CSV)\n",
    "        for c in (\"step\",\"train_loss\",\"eval_loss\"):\n",
    "            if c in df_metrics.columns:\n",
    "                df_metrics[c] = pd.to_numeric(df_metrics[c], errors=\"coerce\")\n",
    "        last_train = df_metrics[\"train_loss\"].dropna().iloc[-1] if \"train_loss\" in df_metrics and df_metrics[\"train_loss\"].notna().any() else None\n",
    "        last_eval  = df_metrics[\"eval_loss\"].dropna().iloc[-1]  if \"eval_loss\" in df_metrics and df_metrics[\"eval_loss\"].notna().any() else None\n",
    "        if last_train is not None: mlflow.log_metric(\"train_loss_last\", float(last_train))\n",
    "        if last_eval  is not None: mlflow.log_metric(\"eval_loss_last\",  float(last_eval))\n",
    "    except Exception as e:\n",
    "        print(\"[MLflow] Aviso: no pude leer métricas CSV:\", e)\n",
    "\n",
    "    # Artefactos\n",
    "    if os.path.exists(METRICS_CSV): mlflow.log_artifact(METRICS_CSV, artifact_path=\"metrics\")\n",
    "    if os.path.exists(PLOT_PATH):    mlflow.log_artifact(PLOT_PATH,    artifact_path=\"plots\")\n",
    "\n",
    "    # Guarda adapters y tokenizer como artefactos\n",
    "    #mlflow.log_artifacts(FINAL_DIR, artifact_path=\"model_final\")\n",
    "\n",
    "print(\"MLflow: experimento registrado. Tracking URI:\", mlflow.get_tracking_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def export_code_cells():\n",
    "    from IPython import get_ipython\n",
    "    cells = get_ipython().user_ns['In']\n",
    "    code = '\\n\\n'.join([c for c in cells if c.strip()])\n",
    "    return Markdown(f'```python\\n{code}\\n```')\n",
    "\n",
    "#export_code_cells()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_generacion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
