{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ada16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, time, json, random, csv, gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    ")\n",
    "from transformers.trainer_callback import TrainerCallback, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURACIÓN\n",
    "# ----------------------------\n",
    "MODEL_ID = os.getenv(\"MODEL_ID\", \"meta-llama/Llama-3.2-3B-Instruct\")   \n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)                                  \n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CUT_OFF_LEN     = int(os.getenv(\"CUT_OFF_LEN\", \"1024\"))   # longitud máxima del texto de entrada (tokens)\n",
    "MAX_NEW_TOKENS  = int(os.getenv(\"MAX_NEW_TOKENS\", \"512\")) # límite de tokens que puede generar el modelo\n",
    "TEMPERATURE     = float(os.getenv(\"TEMPERATURE\", \"0.2\"))  # controla la aleatoriedad (más bajo = más determinista)\n",
    "TOP_P           = float(os.getenv(\"TOP_P\", \"0.9\"))        # limita la generación a los tokens más probables (≈TOP_P % de probabilidad acumulada)\n",
    "N_TRAIN         = int(os.getenv(\"N_TRAIN\", \"1500\"))       # número de muestras de entrenamiento (subconjunto)\n",
    "EVAL_STEPS      = int(os.getenv(\"EVAL_STEPS\", \"10\"))      # cada cuántos pasos se evalúa el modelo\n",
    "LOG_STEPS       = int(os.getenv(\"LOG_STEPS\", \"10\"))       # cada cuántos pasos se registran métricas (logs)\n",
    "SAVE_STEPS      = int(os.getenv(\"SAVE_STEPS\", \"100\"))     # cada cuántos pasos se guarda un checkpoint\n",
    "NUM_EPOCHS      = int(os.getenv(\"NUM_EPOCHS\", \"3\"))       # número total de épocas de entrenamiento\n",
    "BATCH_TRAIN     = int(os.getenv(\"BATCH_TRAIN\", \"2\"))      # tamaño de lote (batch) para entrenamiento\n",
    "BATCH_EVAL      = int(os.getenv(\"BATCH_EVAL\", \"2\"))       # tamaño de lote (batch) para validación\n",
    "GRAD_ACC_STEPS  = int(os.getenv(\"GRAD_ACC_STEPS\", \"16\"))  # pasos para acumular gradientes (simula batch grande)\n",
    "LR              = float(os.getenv(\"LR\", \"1e-4\"))          # tasa de aprendizaje\n",
    "WARMUP_RATIO    = float(os.getenv(\"WARMUP_RATIO\", \"0.05\"))# fracción inicial del entrenamiento usada para warmup\n",
    "SEED            = int(os.getenv(\"SEED\", \"42\"))            # semilla aleatoria para reproducibilidad\n",
    "\n",
    "# LoRA \n",
    "LORA_R          = int(os.getenv(\"LORA_R\", \"16\"))          # rango de la descomposición de matrices\n",
    "LORA_ALPHA      = int(os.getenv(\"LORA_ALPHA\", \"32\"))      # escala del aprendizaje en capas LoRA\n",
    "LORA_DROPOUT    = float(os.getenv(\"LORA_DROPOUT\", \"0.05\"))# probabilidad de dropout en LoRA\n",
    "\n",
    "# Métricas\n",
    "LOSS_WEIGHTS    = [0.3, 0.3, 0.2, 0.1, 0.1]\n",
    "\n",
    "# Salidas\n",
    "SAFE_MODEL_NAME = MODEL_ID.replace(\"/\", \"__\")\n",
    "OUTPUT_DIR   = os.getenv(\"OUTPUT_DIR\", f\"outputs/{SAFE_MODEL_NAME}-lora-fp16\")\n",
    "METRICS_CSV  = f\"{OUTPUT_DIR}/train_val_metrics.csv\"\n",
    "PLOT_PATH    = f\"{OUTPUT_DIR}/loss_curves.png\"\n",
    "FINAL_DIR    = f\"{OUTPUT_DIR}/final\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_DIR, exist_ok=True)\n",
    "\n",
    "# Otros\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "print(\"Device:\", DEVICE, \"| GPU:\", torch.cuda.get_device_name(0) if DEVICE==\"cuda\" else \"CPU\")\n",
    "print(\"Model:\", MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc838e5",
   "metadata": {},
   "source": [
    "## Configurar el wrapper de métricas y funcion de pérdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb516b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca la carpeta 'metricas' hacia arriba y añade a sys.path\n",
    "HERE = Path.cwd()\n",
    "found = None\n",
    "for p in [HERE] + list(HERE.parents):\n",
    "    if (p / \"metricas\").is_dir():\n",
    "        found = p\n",
    "        break\n",
    "if not found:\n",
    "    raise RuntimeError(\"No se encontró la carpeta 'metricas' en ningún ancestro.\")\n",
    "if str(found) not in sys.path:\n",
    "    sys.path.insert(0, str(found))\n",
    "\n",
    "from metricas.metrics_client import getLoss  # usa http://localhost:8000\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_composite_loss(model, tokenizer, val_df, sample_size=64):\n",
    "    if len(val_df) == 0:\n",
    "        return None\n",
    "    sub = val_df.sample(n=min(sample_size, len(val_df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "             {\"role\": \"user\",   \"content\": USER_PREFIX + s}],\n",
    "            tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        for s in sub[\"source_text\"].tolist()\n",
    "    ]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=CUT_OFF_LEN).to(DEVICE)\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        eos_token_id=EOS_ID,                 \n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    outs = tokenizer.batch_decode(gen[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    losses = getLoss(\n",
    "        sub[\"source_text\"].tolist(),\n",
    "        sub[\"target_text\"].tolist(),\n",
    "        outs,\n",
    "        weights=LOSS_WEIGHTS\n",
    "    )\n",
    "    if isinstance(losses, list):\n",
    "        return float(sum(losses)/len(losses))\n",
    "    return float(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb01dd",
   "metadata": {},
   "source": [
    "## Carga y repartición de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed168056",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = os.getenv(\"CSV_PATH\", \"../../data/pls_abstract_pairs_with_metrics.csv\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Split en val/test\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "if N_TRAIN:\n",
    "    train_df = train_df.sample(n=min(N_TRAIN, len(train_df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df)} pares \\nVal: {len(val_df)} pares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574690c",
   "metadata": {},
   "source": [
    "## Definir Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d000783",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You simplify clinical trial protocol text into a plain-language summary for the general public. \"\n",
    "    \"Keep to 6–8th grade readability, avoid diagnoses and speculation, no hallucinations, \"\n",
    "    \"and preserve key facts (objective, population, interventions, outcomes, timelines, safety).\"\n",
    ")\n",
    "USER_PREFIX = \"Using the following clinical trial protocol text as input, create a plain language summary.\\n\\n\"\n",
    "\n",
    "def build_chat(src: str, tgt: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PREFIX + str(src)},\n",
    "        {\"role\": \"assistant\", \"content\": str(tgt).rstrip() + \" <|sentence_end|>\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "def encode_supervised(batch, tokenizer):\n",
    "    out = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "    # Presupuesto duro para que el EOS entre\n",
    "    max_len = CUT_OFF_LEN\n",
    "    eos_id = tokenizer.convert_tokens_to_ids(\"<|sentence_end|>\")\n",
    "\n",
    "    for s, t in zip(batch[\"source_text\"], batch[\"target_text\"]):\n",
    "        # Intentamos con el src completo y, si el EOS queda fuera, recortamos el src\n",
    "        src = str(s)\n",
    "        tgt = str(t)\n",
    "\n",
    "        for _ in range(5):  # hasta 5 intentos de recorte\n",
    "            chat = build_chat(src, tgt)\n",
    "            text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "            toks = tokenizer(text, truncation=True, max_length=max_len, padding=False)\n",
    "\n",
    "            # ¿Entró el EOS?\n",
    "            if eos_id in toks[\"input_ids\"]:\n",
    "                out[\"input_ids\"].append(toks[\"input_ids\"])\n",
    "                out[\"attention_mask\"].append(toks[\"attention_mask\"])\n",
    "                break\n",
    "\n",
    "            # Si no entró, recorta el src (recorte conservador al ~85%)\n",
    "            if len(src) < 100:\n",
    "                # ya está muy corto; lo aceptamos tal cual para no perder ejemplo\n",
    "                out[\"input_ids\"].append(toks[\"input_ids\"])\n",
    "                out[\"attention_mask\"].append(toks[\"attention_mask\"])\n",
    "                break\n",
    "            src = src[: int(len(src) * 0.85)]\n",
    "        else:\n",
    "            # Si no se logró en 5 intentos, registramos igualmente (raro)\n",
    "            out[\"input_ids\"].append(toks[\"input_ids\"])\n",
    "            out[\"attention_mask\"].append(toks[\"attention_mask\"])\n",
    "\n",
    "    return out\n",
    "# tokeniza, verifica si el eos_id aparece en input_ids. Si no, recorta source_text y vuelve a intentar hasta que entre. No toca el target (lo preserva completo con el EOS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_id: str, hf_token: str | None = HF_TOKEN):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True, token=hf_token, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"right\"\n",
    "    return tok\n",
    "\n",
    "def load_causallm(model_id: str, device: str = DEVICE, hf_token: str | None = HF_TOKEN):\n",
    "    # Verifica que sea un modelo de lenguaje causal compatible (no VL)\n",
    "    try:\n",
    "        cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=True, token=hf_token)\n",
    "        arch_ok = any(\"CausalLM\" in a for a in getattr(cfg, \"architectures\", []) or [])\n",
    "    except Exception:\n",
    "        arch_ok = True  # algunos repos no exponen architectures; intentamos cargar de todas formas\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16 if device.startswith(\"cuda\") else torch.float32,\n",
    "        trust_remote_code=True, token=hf_token\n",
    "    ).to(device)\n",
    "\n",
    "    # Si no es CausalLM, esto normalmente falla al forward; advertimos:\n",
    "    if not arch_ok:\n",
    "        print(\"[WARN] El repo no declara arquitectura CausalLM. Si falla el forward, usar un modelo TEXT (no VL).\")\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    return model\n",
    "\n",
    "tokenizer = load_tokenizer(MODEL_ID)\n",
    "model     = load_causallm(MODEL_ID)\n",
    "special = {\"additional_special_tokens\": [\"<|sentence_end|>\"]}\n",
    "added = tokenizer.add_special_tokens(special)\n",
    "if added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "EOS_ID = tokenizer.convert_tokens_to_ids(\"<|sentence_end|>\")\n",
    "print(\"Tokenizer & model listos. Device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22356315",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = Dataset.from_pandas(train_df[[\"source_text\",\"target_text\"]]).map(\n",
    "    lambda b: encode_supervised(b, tokenizer), batched=True, remove_columns=[\"source_text\",\"target_text\"]\n",
    ")\n",
    "hf_val = Dataset.from_pandas(val_df[[\"source_text\",\"target_text\"]]).map(\n",
    "    lambda b: encode_supervised(b, tokenizer), batched=True, remove_columns=[\"source_text\",\"target_text\"]\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "print(hf_train, hf_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d797453",
   "metadata": {},
   "source": [
    "## LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a691d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_find_lora_targets(model, extra_patterns=None):\n",
    "    common = {\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "        \"wi\",\"wo\",\"wq\",\"wk\",\"wv\",\"W_pack\",\"query_key_value\"\n",
    "    }\n",
    "    if extra_patterns:\n",
    "        common |= set(extra_patterns)\n",
    "    found = set()\n",
    "    for name, module in model.named_modules():\n",
    "        base = name.split(\".\")[-1]\n",
    "        if base in common:\n",
    "            found.add(base)\n",
    "    if not found:\n",
    "        found = {\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"}\n",
    "    return sorted(found)\n",
    "\n",
    "targets = auto_find_lora_targets(model)\n",
    "print(\"LoRA targets:\", targets)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=targets\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.train()\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable/1e6:.2f}M / {total/1e6:.2f}M ({100*trainable/total:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c185436",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39149ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValCSVLogger(TrainerCallback):\n",
    "    def __init__(self, csv_path=METRICS_CSV):\n",
    "        self.csv_path = csv_path\n",
    "        os.makedirs(os.path.dirname(self.csv_path), exist_ok=True)\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
    "                csv.writer(f).writerow([\"step\",\"train_loss\",\"eval_loss\",\"lr\",\"timestamp\",\"composite_loss\"])\n",
    "\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        logs = kwargs.get(\"logs\", {})\n",
    "        step = state.global_step\n",
    "        tl   = logs.get(\"loss\")\n",
    "        el   = logs.get(\"eval_loss\")\n",
    "        lr   = logs.get(\"learning_rate\")\n",
    "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([step, tl, el, lr, time.time(), None])\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        comp = eval_composite_loss(kwargs[\"model\"], tokenizer, val_df)\n",
    "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([state.global_step, None, kwargs[\"metrics\"].get(\"eval_loss\"), None, time.time(), comp])\n",
    "\n",
    "early_stop = EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=1e-4)\n",
    "csv_logger = TrainValCSVLogger(METRICS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8bef3",
   "metadata": {},
   "source": [
    "## PEFT (Ajuste Fino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ec879",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    fp16=(DEVICE==\"cuda\"),\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.add_callback(csv_logger)\n",
    "trainer.add_callback(early_stop)\n",
    "\n",
    "approx_steps = math.ceil(len(hf_train) / max(1, BATCH_TRAIN) / max(1, GRAD_ACC_STEPS)) * NUM_EPOCHS\n",
    "print(\"Aprox. training steps:\", approx_steps)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90486d4",
   "metadata": {},
   "source": [
    "## Guardado y demo de inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b337f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado final (PEFT adapters + tokenizer)\n",
    "trainer.model.save_pretrained(FINAL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_DIR)\n",
    "print(\"Guardado en:\", FINAL_DIR)\n",
    "\n",
    "# Demo de inferencia corta y estable (greedy + anti-repetición)\n",
    "demo_src = val_df.iloc[0][\"source_text\"]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    [{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "     {\"role\":\"user\",\"content\":USER_PREFIX + demo_src}],\n",
    "    tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(demo_src)\n",
    "\n",
    "print(\"\\n\\n\", \"RESUMEN GENERADO POR IA:\", \"\\n\\n\")\n",
    "\n",
    "model.eval()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=min(MAX_NEW_TOKENS, 256),\n",
    "        do_sample=False,\n",
    "        no_repeat_ngram_size=6,\n",
    "        repetition_penalty=1.15,\n",
    "        eos_token_id=EOS_ID,              \n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "decoded = tokenizer.decode(gen[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f0752",
   "metadata": {},
   "source": [
    "## Gráfica de rendimiento (entrenamiento v.s. validación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.read_csv(METRICS_CSV)\n",
    "for c in (\"step\",\"train_loss\",\"eval_loss\",\"lr\"):\n",
    "    if c in dfm.columns: dfm[c] = pd.to_numeric(dfm[c], errors=\"coerce\")\n",
    "dfm = dfm.dropna(subset=[\"train_loss\",\"eval_loss\",\"lr\"], how=\"all\")\n",
    "dfm = dfm.sort_values(\"step\").groupby(\"step\", as_index=False).last()\n",
    "\n",
    "xs_tr = dfm[\"step\"].to_numpy()\n",
    "ys_tr = dfm[\"train_loss\"].to_numpy()\n",
    "xs_ev = dfm[\"step\"].to_numpy()\n",
    "ys_ev = dfm[\"eval_loss\"].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "if np.isfinite(ys_tr).any(): plt.plot(xs_tr, ys_tr, \"-o\", label=\"train_loss\", lw=2, ms=4)\n",
    "if np.isfinite(ys_ev).any(): plt.plot(xs_ev, ys_ev, \"-o\", label=\"eval_loss\",  lw=2, ms=4)\n",
    "\n",
    "vals = np.concatenate([a[~np.isnan(a)] for a in [ys_tr, ys_ev] if len(a)])\n",
    "if len(vals):\n",
    "    pad = max(1e-4, 0.08*(vals.max()-vals.min()))\n",
    "    plt.ylim(vals.min()-pad, vals.max()+pad)\n",
    "\n",
    "plt.title(f\"Training vs Validation Loss — {SAFE_MODEL_NAME}\")\n",
    "plt.xlabel(\"step\"); plt.ylabel(\"loss\"); plt.title(\"Training vs Validation Loss\")\n",
    "plt.grid(alpha=0.3); plt.legend()\n",
    "plt.savefig(PLOT_PATH, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot guardado en:\", PLOT_PATH)\n",
    "print(\"Métricas CSV:\", METRICS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2c295",
   "metadata": {},
   "source": [
    "## Experimento (mlflow -> Databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c22411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Configuración MLflow\n",
    "# # - Local: exporta MLFLOW_TRACKING_URI=\"file:./mlruns\"  (por defecto mlflow lo hace local)\n",
    "# # - Databricks: exporta MLFLOW_TRACKING_URI=\"databricks\" y los tokens (DATABRICKS_HOST, DATABRICKS_TOKEN)\n",
    "# mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns\"))\n",
    "# mlflow.set_experiment(os.getenv(\"MLFLOW_EXPERIMENT\", f\"/{SAFE_MODEL_NAME}\"))\n",
    "\n",
    "# run_name = f\"{SAFE_MODEL_NAME}-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# with mlflow.start_run(run_name=run_name):\n",
    "#     # Params clave\n",
    "#     mlflow.log_params({\n",
    "#         \"model_id\": MODEL_ID,\n",
    "#         \"fp16\": (DEVICE==\"cuda\"),\n",
    "#         \"cut_off_len\": CUT_OFF_LEN,\n",
    "#         \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "#         \"temperature\": TEMPERATURE,\n",
    "#         \"top_p\": TOP_P,\n",
    "#         \"epochs\": NUM_EPOCHS,\n",
    "#         \"batch_train\": BATCH_TRAIN,\n",
    "#         \"batch_eval\": BATCH_EVAL,\n",
    "#         \"grad_acc_steps\": GRAD_ACC_STEPS,\n",
    "#         \"lr\": LR,\n",
    "#         \"warmup_ratio\": WARMUP_RATIO,\n",
    "#         \"seed\": SEED,\n",
    "#         \"lora_r\": LORA_R,\n",
    "#         \"lora_alpha\": LORA_ALPHA,\n",
    "#         \"lora_dropout\": LORA_DROPOUT,\n",
    "#         \"lora_targets\": json.dumps(auto_find_lora_targets(model), ensure_ascii=False),\n",
    "#         \"loss_weights\": json.dumps(LOSS_WEIGHTS),\n",
    "#         \"n_train_rows\": len(train_df),\n",
    "#         \"n_val_rows\": len(val_df),\n",
    "#     })\n",
    "\n",
    "#     # Métricas finales (si existen en CSV)\n",
    "#     try:\n",
    "#         df_metrics = pd.read_csv(METRICS_CSV)\n",
    "#         for c in (\"step\",\"train_loss\",\"eval_loss\"):\n",
    "#             if c in df_metrics.columns:\n",
    "#                 df_metrics[c] = pd.to_numeric(df_metrics[c], errors=\"coerce\")\n",
    "#         last_train = df_metrics[\"train_loss\"].dropna().iloc[-1] if \"train_loss\" in df_metrics and df_metrics[\"train_loss\"].notna().any() else None\n",
    "#         last_eval  = df_metrics[\"eval_loss\"].dropna().iloc[-1]  if \"eval_loss\" in df_metrics and df_metrics[\"eval_loss\"].notna().any() else None\n",
    "#         if last_train is not None: mlflow.log_metric(\"train_loss_last\", float(last_train))\n",
    "#         if last_eval  is not None: mlflow.log_metric(\"eval_loss_last\",  float(last_eval))\n",
    "#     except Exception as e:\n",
    "#         print(\"[MLflow] Aviso: no pude leer métricas CSV:\", e)\n",
    "\n",
    "#     # Artefactos\n",
    "#     if os.path.exists(METRICS_CSV): mlflow.log_artifact(METRICS_CSV, artifact_path=\"metrics\")\n",
    "#     if os.path.exists(PLOT_PATH):    mlflow.log_artifact(PLOT_PATH,    artifact_path=\"plots\")\n",
    "\n",
    "#     # Guarda adapters y tokenizer como artefactos\n",
    "#     mlflow.log_artifacts(FINAL_DIR, artifact_path=\"model_final\")\n",
    "\n",
    "# print(\"MLflow: experimento registrado. Tracking URI:\", mlflow.get_tracking_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def export_code_cells():\n",
    "    from IPython import get_ipython\n",
    "    cells = get_ipython().user_ns['In']\n",
    "    code = '\\n\\n'.join([c for c in cells if c.strip()])\n",
    "    return Markdown(f'```python\\n{code}\\n```')\n",
    "\n",
    "#export_code_cells()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_generacion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
