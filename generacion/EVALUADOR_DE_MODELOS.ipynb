{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997f217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time, random, sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from math import ceil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(str(Path.cwd().parents[0])) \n",
    "import KEYS\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Métricas\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from metricas.metrics_client import getRelevance, getFactuality, getReadability\n",
    "\n",
    "# Reproducibilidad\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \", DEVICE)\n",
    "\n",
    "# Prompts (usa los mismos que en entrenamiento)\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You simplify clinical trial protocol text into a plain-language summary for the general public. \"\n",
    "#     \"Keep to 6–8th grade readability, avoid diagnoses and speculation, no hallucinations, \"\n",
    "#     \"and preserve key facts (objective, population, interventions, outcomes, timelines, safety).\"\n",
    "# )\n",
    "# USER_PREFIX = \"Using the following clinical trial protocol text as input, create a plain language summary.\\n\\n\"\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert scientific writer specialized in simplifying clinical trial protocols. \"\n",
    "    \"Your task is to rewrite technical protocol content into a clear, accurate, and accessible plain-language summary \"\n",
    "    \"for a general audience. Maintain a 6th–8th grade reading level, ensure factual accuracy, and strictly avoid \"\n",
    "    \"hallucinations, interpretations, or medical advice. The summary must clearly cover the study’s objective, population, \"\n",
    "    \"interventions, outcomes, timeline, and safety information, using concise and neutral language.\"\n",
    ")\n",
    "\n",
    "USER_PREFIX = (\n",
    "    \"Below is a section from a clinical trial protocol. Rewrite it as a plain-language summary that is accurate, clear, \"\n",
    "    \"and easy to understand for the public while preserving all essential details.\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb049bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_dir: str, device: str = DEVICE):\n",
    "    model_dir = str(Path(model_dir).resolve())\n",
    "    cfg_path = Path(model_dir) / \"adapter_config.json\"\n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"No existe {cfg_path}\")\n",
    "\n",
    "    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        adapter_cfg = json.load(f)\n",
    "    base = adapter_cfg.get(\"base_model_name_or_path\")\n",
    "    if not base:\n",
    "        raise ValueError(\"adapter_config.json no contiene 'base_model_name_or_path'.\")\n",
    "\n",
    "    # Tokenizer desde el directorio del adapter (incluye el token extra)\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"left\"\n",
    "\n",
    "    # Modelo base\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base,\n",
    "        torch_dtype=torch.float16 if device.startswith(\"cuda\") else torch.float32,\n",
    "        trust_remote_code=True\n",
    "    ).to(device)\n",
    "\n",
    "    #  alinear tamaño de vocab del base al del tokenizer usado en el adapter\n",
    "    base_model.resize_token_embeddings(len(tok))\n",
    "\n",
    "    # Cargar adapters LoRA\n",
    "    model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "    model.eval()\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    # token de fin de oración personalizado\n",
    "    eos_id = None\n",
    "    try:\n",
    "        eid = tok.convert_tokens_to_ids(\"<|sentence_end|>\")\n",
    "        if eid is not None and eid != tok.unk_token_id:\n",
    "            eos_id = eid\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tok, eos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1557a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS_ID (custom): 151665\n"
     ]
    }
   ],
   "source": [
    "# === Parámetros de evaluación ===\n",
    "MODEL_DIR = \"Qwen/outputs/Qwen__Qwen2.5-7B-Instruct-7B_FK8/final\"\n",
    "DATASET_CSV = \"../data/pls_abstract_pairs_with_metrics.csv\"\n",
    "SPLIT = \"test\"                  # 'test' o 'train'\n",
    "NUM_EXAMPLES = 150               # cuántos ejemplos del split\n",
    "SAMPLES_PER_EXAMPLE = 2         # cuántos resúmenes por ejemplo\n",
    "\n",
    "SEED = 44\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# generación \n",
    "GEN_CFG = dict(\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.05,\n",
    ")\n",
    "\n",
    "# Parametros de ventana deslizante\n",
    "WINDOW_TOKENS = 512\n",
    "OVERLAP_TOKENS = 128    # solape entre ventanas\n",
    "\n",
    "# batches\n",
    "BATCH_GEN = 2           # prompts por batch para generate()\n",
    "BATCH_METRICS = 4       # micro-lote por POST a la API de métricas\n",
    "\n",
    "# servicio de métricas\n",
    "METRICS_URL = \"http://127.0.0.1:8000\" # corriendo en local\n",
    "\n",
    "# salida\n",
    "OUT_DIR = \"GENERATED_TEXTS\" \n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "model, tokenizer, EOS_ID = load_model_and_tokenizer(MODEL_DIR, DEVICE)\n",
    "print(\"EOS_ID (custom):\", EOS_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce0bb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === utilidades de sliding window (ENTRADA) ===\n",
    "def _template_tokens_cost() -> int:\n",
    "    # mide tokens de template con un dummy corto\n",
    "    dummy = \"X\"\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "         {\"role\":\"user\",\"content\":USER_PREFIX + dummy}],\n",
    "        tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return len(tokenizer(prompt, add_special_tokens=False).input_ids)\n",
    "\n",
    "def windowize_source(text: str, window_tokens: int, overlap_tokens: int) -> List[str]:\n",
    "    ids = tokenizer(text, add_special_tokens=False).input_ids\n",
    "    if len(ids) <= window_tokens:\n",
    "        return [text]\n",
    "    windows, step = [], max(1, window_tokens - overlap_tokens)\n",
    "    for start in range(0, len(ids), step):\n",
    "        end = min(start + window_tokens, len(ids))\n",
    "        chunk = tokenizer.decode(ids[start:end], skip_special_tokens=True).strip()\n",
    "        if chunk:\n",
    "            windows.append(chunk)\n",
    "        if end == len(ids):\n",
    "            break\n",
    "    return windows\n",
    "\n",
    "def build_prompt(src: str) -> str:\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "         {\"role\":\"user\",\"content\":USER_PREFIX + str(src)}],\n",
    "        tokenize=False, add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc78e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generación con ventana deslizante en ENTRADA y fusión de parciales ===\n",
    "@torch.no_grad()\n",
    "def generate_samples(sliding_sources: List[List[str]], n_samples: int) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    `sliding_sources`: lista de documentos; cada documento es una lista de ventanas de *source* (texto).\n",
    "    Retorna: lista de documentos; cada doc es lista de 'n_samples' salidas (cada salida es fusión de parciales).\n",
    "    \"\"\"\n",
    "    all_doc_outputs = [list() for _ in range(len(sliding_sources))]\n",
    "    base_cfg = GEN_CFG.copy()\n",
    "    # Usar ambos EOS si existe custom\n",
    "    if EOS_ID is not None:\n",
    "        base_cfg[\"eos_token_id\"] = [tokenizer.eos_token_id, EOS_ID]\n",
    "    else:\n",
    "        base_cfg[\"eos_token_id\"] = tokenizer.eos_token_id\n",
    "    base_cfg[\"pad_token_id\"] = tokenizer.pad_token_id\n",
    "\n",
    "    # Para mostrar progreso: nº de rondas × nº de docs × nº de lotes por doc (aprox por ventanas)\n",
    "    total_windows = sum(len(wins) for wins in sliding_sources)\n",
    "    approx_batches_per_round = ceil(total_windows / max(1, BATCH_GEN))\n",
    "    pbar = tqdm(total=n_samples * approx_batches_per_round, desc=\"Generando : \", unit=\"batch\")\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # generamos documento por documento para poder fusionar ventanas\n",
    "        for doc_idx, wins in enumerate(sliding_sources):\n",
    "            if not wins:\n",
    "                all_doc_outputs[doc_idx].append(\"\")  # doc vacío\n",
    "                continue\n",
    "\n",
    "            # construir prompts por ventana\n",
    "            prompts = [build_prompt(w) for w in wins]\n",
    "\n",
    "            # generar por lotes de ventanas\n",
    "            partials = []\n",
    "            for i in range(0, len(prompts), BATCH_GEN):\n",
    "                j = min(i + BATCH_GEN, len(prompts))\n",
    "                batch_prompts = prompts[i:j]\n",
    "                inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "                gen = model.generate(**inputs, **base_cfg)\n",
    "                cut = inputs[\"input_ids\"].shape[1]\n",
    "                outs = tokenizer.batch_decode(gen[:, cut:], skip_special_tokens=True)\n",
    "                partials.extend([o.strip() for o in outs])\n",
    "                pbar.update(1)\n",
    "\n",
    "            # fusión (idéntica a lo discutido en entrenamiento: concatenación simple)\n",
    "            fused = \"\\n\".join(p for p in partials if p)\n",
    "            all_doc_outputs[doc_idx].append(fused)\n",
    "\n",
    "    pbar.close()\n",
    "    return all_doc_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec59ef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando :  35%|███▌      | 163/460 [5:36:08<9:07:03, 110.52s/batch] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Crear ventanas por documento (\u001b[39;00m\n\u001b[0;32m     16\u001b[0m sliding_sources \u001b[38;5;241m=\u001b[39m [windowize_source(src, WINDOW_TOKENS, OVERLAP_TOKENS) \u001b[38;5;28;01mfor\u001b[39;00m src \u001b[38;5;129;01min\u001b[39;00m sources]\n\u001b[1;32m---> 18\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43msliding_sources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAMPLES_PER_EXAMPLE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[5], line 38\u001b[0m, in \u001b[0;36mgenerate_samples\u001b[1;34m(sliding_sources, n_samples)\u001b[0m\n\u001b[0;32m     36\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i:j]\n\u001b[0;32m     37\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(batch_prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 38\u001b[0m gen \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbase_cfg)\n\u001b[0;32m     39\u001b[0m cut \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     40\u001b[0m outs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(gen[:, cut:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\peft\\peft_model.py:1973\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1971\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1972\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1973\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1975\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2566\u001b[0m     input_ids,\n\u001b[0;32m   2567\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2568\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2569\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2572\u001b[0m )\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m ):\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\generation\\utils.py:2784\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2781\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[1;32m-> 2784\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:449\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[0;32m    431\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[0;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    450\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    451\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    452\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    453\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    454\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    455\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    456\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    458\u001b[0m     )\n\u001b[0;32m    460\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:384\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[1;32m--> 384\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    385\u001b[0m         hidden_states,\n\u001b[0;32m    386\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask_mapping[decoder_layer\u001b[38;5;241m.\u001b[39mattention_type],\n\u001b[0;32m    387\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    388\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    389\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    390\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    391\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    393\u001b[0m     )\n\u001b[0;32m    395\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[0;32m    397\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    398\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    399\u001b[0m )\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:234\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    235\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    236\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    237\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    238\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    239\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    240\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    241\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    243\u001b[0m )\n\u001b[0;32m    244\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:169\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    167\u001b[0m     attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[1;32m--> 169\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    171\u001b[0m     query_states,\n\u001b[0;32m    172\u001b[0m     key_states,\n\u001b[0;32m    173\u001b[0m     value_states,\n\u001b[0;32m    174\u001b[0m     attention_mask,\n\u001b[0;32m    175\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout,\n\u001b[0;32m    176\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    177\u001b[0m     sliding_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window,  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    179\u001b[0m )\n\u001b[0;32m    181\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    182\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[1;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool:\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_not(attention_mask\u001b[38;5;241m.\u001b[39mbool())\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 96\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m     97\u001b[0m     query,\n\u001b[0;32m     98\u001b[0m     key,\n\u001b[0;32m     99\u001b[0m     value,\n\u001b[0;32m    100\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    101\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[0;32m    102\u001b[0m     scale\u001b[38;5;241m=\u001b[39mscaling,\n\u001b[0;32m    103\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msdpa_kwargs,\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    106\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_CSV)\n",
    "cols = {\"source_text\",\"target_text\",\"split\"}\n",
    "if not cols.issubset(df.columns):\n",
    "    raise ValueError(f\"El CSV debe contener {cols}\")\n",
    "\n",
    "val_df = df[df[\"split\"] == SPLIT].dropna(subset=[\"source_text\",\"target_text\"]).reset_index(drop=True)\n",
    "if len(val_df) == 0:\n",
    "    raise ValueError(f\"No hay filas para split='{SPLIT}'\")\n",
    "if NUM_EXAMPLES < len(val_df):\n",
    "    val_df = val_df.sample(n=NUM_EXAMPLES, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "sources = val_df[\"source_text\"].tolist()\n",
    "targets = val_df[\"target_text\"].tolist()\n",
    "\n",
    "# Crear ventanas por documento (\n",
    "sliding_sources = [windowize_source(src, WINDOW_TOKENS, OVERLAP_TOKENS) for src in sources]\n",
    "\n",
    "generated = generate_samples(sliding_sources, SAMPLES_PER_EXAMPLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(generated) == len(sliding_sources)\n",
    "assert all(len(xs) == SAMPLES_PER_EXAMPLE for xs in generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489015b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked(iterable, n):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]\n",
    "\n",
    "rows = []  # filas por ejemplo×muestra\n",
    "for doc_id, (src, tgt, gens) in enumerate(zip(sources, targets, generated)):\n",
    "    # evaluamos por micro-lotes contra la API\n",
    "    pairs_src, pairs_gen = [], []\n",
    "    for sidx, gen_txt in enumerate(gens):\n",
    "        pairs_src.append(src)          # siempre contra el documento completo\n",
    "        pairs_gen.append(gen_txt)      # salida final fusionada\n",
    "\n",
    "    # relevance\n",
    "    rel_scores = []\n",
    "    for c_src, c_gen in zip(chunked(pairs_src, BATCH_METRICS), chunked(pairs_gen, BATCH_METRICS)):\n",
    "        rel_scores.extend(getRelevance(c_src, c_gen, base_url=METRICS_URL, timeout=180.0))\n",
    "\n",
    "    # factuality\n",
    "    fac_scores = []\n",
    "    for c_src, c_gen in zip(chunked(pairs_src, BATCH_METRICS), chunked(pairs_gen, BATCH_METRICS)):\n",
    "        fac_scores.extend(getFactuality(c_src, c_gen, base_url=METRICS_URL, timeout=900.0))\n",
    "\n",
    "    # readability (solo sobre generados)\n",
    "    fkgl_all, smog_all, dale_all = [], [], []\n",
    "    for c_gen in chunked(pairs_gen, BATCH_METRICS):\n",
    "        rd = getReadability(c_gen, base_url=METRICS_URL, timeout=180.0)\n",
    "        fkgl_all.extend(rd[\"fkgl\"]); smog_all.extend(rd[\"smog\"]); dale_all.extend(rd[\"dale_chall\"])\n",
    "\n",
    "    # registrar filas\n",
    "    for sidx in range(len(gens)):\n",
    "        rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sample_idx\": sidx,\n",
    "            \"relevance\": float(rel_scores[sidx]),\n",
    "            \"factuality\": float(fac_scores[sidx]),\n",
    "            \"fkgl\": float(fkgl_all[sidx]),\n",
    "            \"smog\": float(smog_all[sidx]),\n",
    "            \"dale_chall\": float(dale_all[sidx]),\n",
    "        })\n",
    "\n",
    "per_sample_df = pd.DataFrame(rows).sort_values([\"doc_id\",\"sample_idx\"]).reset_index(drop=True)\n",
    "per_sample_df.head(), per_sample_df.shape\n",
    "\n",
    "def summarize_series(x: pd.Series) -> Dict[str, float]:\n",
    "    v = x.dropna().to_numpy()\n",
    "    if v.size == 0:\n",
    "        return {\"mean\": None,\"std\": None,\"min\": None,\"max\": None,\"p25\": None,\"p50\": None,\"p75\": None,\"n\": 0}\n",
    "    return {\n",
    "        \"mean\": float(np.mean(v)),\n",
    "        \"std\":  float(np.std(v, ddof=1)) if v.size > 1 else 0.0,\n",
    "        \"min\":  float(np.min(v)),\n",
    "        \"max\":  float(np.max(v)),\n",
    "        \"p25\":  float(np.percentile(v, 25)),\n",
    "        \"p50\":  float(np.percentile(v, 50)),\n",
    "        \"p75\":  float(np.percentile(v, 75)),\n",
    "        \"n\":    int(v.size),\n",
    "    }\n",
    "\n",
    "# Agregación global\n",
    "stats = {\n",
    "    \"relevance\":   summarize_series(per_sample_df[\"relevance\"]),\n",
    "    \"factuality\":  summarize_series(per_sample_df[\"factuality\"]),\n",
    "    \"fkgl\":        summarize_series(per_sample_df[\"fkgl\"]),\n",
    "    \"smog\":        summarize_series(per_sample_df[\"smog\"]),\n",
    "    \"dale_chall\":  summarize_series(per_sample_df[\"dale_chall\"]),\n",
    "}\n",
    "\n",
    "# Imprimir estadísticas\n",
    "print(\"=== Estadísticas globales ===\")\n",
    "for k, s in stats.items():\n",
    "    print(f\"\\n{k.upper()}: n={s['n']}\")\n",
    "    print(f\"  mean={s['mean']:.4f}  std={s['std']:.4f}  min={s['min']:.4f}  p25={s['p25']:.4f}  p50={s['p50']:.4f}  p75={s['p75']:.4f}  max={s['max']:.4f}\")\n",
    "\n",
    "# Graficar (una figura por métrica)\n",
    "def plot_distribution(series: pd.Series, title: str):\n",
    "    v = series.dropna().to_numpy()\n",
    "    if v.size == 0:\n",
    "        print(f\"[skip] {title}: sin datos\")\n",
    "        return\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(v, bins=30, alpha=0.7)\n",
    "    plt.title(title); plt.xlabel(\"valor\"); plt.ylabel(\"frecuencia\"); plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.boxplot(v, vert=True, showmeans=True)\n",
    "    plt.title(f\"{title} — boxplot\"); plt.ylabel(\"valor\"); plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_distribution(per_sample_df[\"relevance\"],  \"Relevance\")\n",
    "plot_distribution(per_sample_df[\"factuality\"], \"Factuality\")\n",
    "plot_distribution(per_sample_df[\"fkgl\"],       \"FKGL\")\n",
    "plot_distribution(per_sample_df[\"smog\"],       \"SMOG\")\n",
    "plot_distribution(per_sample_df[\"dale_chall\"], \"Dale-Chall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7684e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar generacion en CSV: primera columna = source_text, siguientes = gen_1..gen_N\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "SAFE_MODEL_NAME = str(MODEL_DIR).replace(os.sep, \"__\").replace(\"/\", \"__\").replace(\" \", \"_\")\n",
    "cols = [\"source_text\"] + [f\"gen_{i+1}\" for i in range(SAMPLES_PER_EXAMPLE)]\n",
    "rows = []\n",
    "\n",
    "for src, gens in zip(sources, generated):\n",
    "    # `generated` es una lista de listas con SAMPLES_PER_EXAMPLE textos por fila\n",
    "    row = [src] + gens\n",
    "    rows.append(row)\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=cols)\n",
    "out_path = Path(OUT_DIR) / f\"{SAFE_MODEL_NAME}_generations.csv\"\n",
    "df_out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"[saved] {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be2cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MLflow (sin cambios, excepto que loguea lo generado tras sliding) ===\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "from mlflow.data import from_pandas\n",
    "\n",
    "os.environ[\"DATABRICKS_HOST\"] = KEYS.DATABRICKS_HOST\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = KEYS.DATABRICKS_TOKEN\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(\"/GeneracionDeResumenes/Estadísticas_de_Modelos\")\n",
    "\n",
    "RUN_OUT = Path(OUT_DIR) / \"mlflow_artifacts\"\n",
    "PLOTS_DIR = RUN_OUT / \"plots\"\n",
    "RUN_OUT.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_distribution(series: pd.Series, title: str, slug: str):\n",
    "    v = series.dropna().to_numpy()\n",
    "    if v.size == 0:\n",
    "        return\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(v, bins=30, alpha=0.7)\n",
    "    plt.title(title); plt.xlabel(\"valor\"); plt.ylabel(\"frecuencia\"); plt.grid(alpha=0.3)\n",
    "    hist_path = PLOTS_DIR / f\"{slug}_hist.png\"\n",
    "    plt.savefig(hist_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.boxplot(v, vert=True, showmeans=True)\n",
    "    plt.title(f\"{title} — boxplot\"); plt.ylabel(\"valor\"); plt.grid(alpha=0.3)\n",
    "    box_path = PLOTS_DIR / f\"{slug}_box.png\"\n",
    "    plt.savefig(box_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "save_distribution(per_sample_df[\"relevance\"],  \"Relevance\",   \"relevance\")\n",
    "save_distribution(per_sample_df[\"factuality\"], \"Factuality\",  \"factuality\")\n",
    "save_distribution(per_sample_df[\"fkgl\"],       \"FKGL\",        \"fkgl\")\n",
    "save_distribution(per_sample_df[\"smog\"],       \"SMOG\",        \"smog\")\n",
    "save_distribution(per_sample_df[\"dale_chall\"], \"Dale-Chall\",  \"dale_chall\")\n",
    "\n",
    "# per_sample_csv = RUN_OUT / \"per_sample_metrics.csv\"\n",
    "# per_sample_df.to_csv(per_sample_csv, index=False)\n",
    "\n",
    "stats_json_path = RUN_OUT / \"aggregate_stats.json\"\n",
    "with open(stats_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stats, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "SAFE_MODEL_NAME = str(MODEL_DIR).replace(os.sep, \"__\").replace(\"/\", \"__\").replace(\" \", \"_\")\n",
    "run_name = f\"{SAFE_MODEL_NAME}-eval-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    try:\n",
    "        ds_val = from_pandas(val_df, source=\"PLS_pairs\", name=f\"PLS_{SPLIT}\")\n",
    "        mlflow.log_input(ds_val, context=\"validation\")\n",
    "    except Exception as e:\n",
    "        print(\"[MLflow] Aviso: no pude registrar dataset de validación:\", e)\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"model_dir\": SAFE_MODEL_NAME,\n",
    "        \"device\": DEVICE,\n",
    "        \"split\": SPLIT,\n",
    "        \"num_examples\": NUM_EXAMPLES,\n",
    "        \"samples_per_example\": SAMPLES_PER_EXAMPLE,\n",
    "        \"System Prompt\": SYSTEM_PROMPT,\n",
    "        \"User_Prefix\": USER_PREFIX,\n",
    "        **{f\"gen_{k}\": v for k, v in GEN_CFG.items()},\n",
    "        \"window_tokens\": WINDOW_TOKENS,\n",
    "        \"overlap_tokens\": OVERLAP_TOKENS,\n",
    "        \"fusion_mode\": \"concat\",\n",
    "    })\n",
    "\n",
    "    mlflow.set_tag(\n",
    "        \"mlflow.note.content\",\n",
    "        f\"Evaluación con Sliding Window en ENTRADA (concat parciales) — split={SPLIT}. \"\n",
    "        f\"{SAMPLES_PER_EXAMPLE} muestras/doc. GenCfg={GEN_CFG}\"\n",
    "    )\n",
    "\n",
    "    def log_agg(prefix: str, d: dict):\n",
    "        clean = {k: float(v) for k, v in d.items() if isinstance(v, (int, float))}\n",
    "        mlflow.log_metrics({f\"{prefix}_{k}\": v for k, v in clean.items()})\n",
    "        if \"n\" in d: mlflow.log_metric(f\"{prefix}_n\", int(d[\"n\"]))\n",
    "    for m in (\"relevance\",\"factuality\",\"fkgl\",\"smog\",\"dale_chall\"):\n",
    "        log_agg(m, stats[m])\n",
    "\n",
    "    \n",
    "    mlflow.log_artifact(str(stats_json_path), artifact_path=\"metrics\")\n",
    "    for p in PLOTS_DIR.glob(\"*.png\"):\n",
    "        mlflow.log_artifact(str(p), artifact_path=\"plots\")\n",
    "\n",
    "    adapter_cfg = Path(MODEL_DIR) / \"adapter_config.json\"\n",
    "    if adapter_cfg.exists():\n",
    "        mlflow.log_artifact(str(adapter_cfg), artifact_path=\"model_info\")\n",
    "\n",
    "    from datetime import datetime as _dt\n",
    "    mlflow.set_tag(\"finished_at\", _dt.utcnow().isoformat() + \"Z\")\n",
    "\n",
    "print(\"MLflow: evaluación registrada. Tracking URI:\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560758b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "def export_code_cells():\n",
    "    from IPython import get_ipython\n",
    "    cells = get_ipython().user_ns['In']\n",
    "    code = '\\n\\n'.join([c for c in cells if c.strip()])\n",
    "    return Markdown(f'```python\\n{code}\\n```')\n",
    "#export_code_cells()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_generacion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
