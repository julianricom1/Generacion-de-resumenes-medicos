{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e108ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, time, random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from math import ceil\n",
    "from tqdm.auto import tqdm\n",
    "import KEYS\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Métricas\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from metricas.metrics_client import getRelevance, getFactuality, getReadability\n",
    "\n",
    "# Reproducibilidad\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Prompts (usa los mismos que en entrenamiento)\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You simplify clinical trial protocol text into a plain-language summary for the general public. \"\n",
    "    \"Keep to 6–8th grade readability, avoid diagnoses and speculation, no hallucinations, \"\n",
    "    \"and preserve key facts (objective, population, interventions, outcomes, timelines, safety).\"\n",
    ")\n",
    "USER_PREFIX = \"Using the following clinical trial protocol text as input, create a plain language summary.\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdfc07",
   "metadata": {},
   "source": [
    "## Función para cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_dir: str, device: str = DEVICE):\n",
    "    model_dir = str(Path(model_dir).resolve())\n",
    "    cfg_path = Path(model_dir) / \"adapter_config.json\"\n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"No existe {cfg_path}\")\n",
    "\n",
    "    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        adapter_cfg = json.load(f)\n",
    "    base = adapter_cfg.get(\"base_model_name_or_path\")\n",
    "    if not base:\n",
    "        raise ValueError(\"adapter_config.json no contiene 'base_model_name_or_path'.\")\n",
    "\n",
    "    # Tokenizer desde el directorio del adapter (incluye el token extra)\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"left\"\n",
    "\n",
    "    # Modelo base\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base,\n",
    "        torch_dtype=torch.float16 if device.startswith(\"cuda\") else torch.float32,\n",
    "        trust_remote_code=True\n",
    "    ).to(device)\n",
    "\n",
    "    #  alinear tamaño de vocab del base al del tokenizer usado en el adapter\n",
    "    base_model.resize_token_embeddings(len(tok))\n",
    "\n",
    "    # Cargar adapters LoRA\n",
    "    model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "    model.eval()\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    # token de fin de oración personalizado\n",
    "    eos_id = None\n",
    "    try:\n",
    "        eid = tok.convert_tokens_to_ids(\"<|sentence_end|>\")\n",
    "        if eid is not None and eid != tok.unk_token_id:\n",
    "            eos_id = eid\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tok, eos_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2b7c1",
   "metadata": {},
   "source": [
    "## Parámetros del proceso de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parámetros de evaluación ===\n",
    "MODEL_DIR = \"ollama/outputs/meta-llama__Llama-3.2-3B-Instruct-6_epocas/final\"\n",
    "DATASET_CSV = \"../data/pls_abstract_pairs_with_metrics.csv\"\n",
    "SPLIT = \"test\"                  # 'test' o 'train'\n",
    "NUM_EXAMPLES = 3               # cuántos ejemplos del split\n",
    "SAMPLES_PER_EXAMPLE = 3         # cuántos resúmenes por ejemplo\n",
    "\n",
    "SEED = 99\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# generación \n",
    "GEN_CFG = dict(\n",
    "    max_new_tokens= 1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    no_repeat_ngram_size=6,\n",
    "    repetition_penalty=1.15,\n",
    ")\n",
    "\n",
    "# batches\n",
    "BATCH_GEN = 2           # prompts por batch para generate()\n",
    "BATCH_METRICS = 4       # micro-lote por POST a la API de métricas\n",
    "\n",
    "# servicio de métricas\n",
    "METRICS_URL = \"http://127.0.0.1:8000\" # corriendo en local\n",
    "\n",
    "# salida\n",
    "OUT_DIR = \"model_evals/llama3.2-3b\" \n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95658c",
   "metadata": {},
   "source": [
    "## Cargar el modelo y los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aaea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, EOS_ID = load_model_and_tokenizer(MODEL_DIR, DEVICE)\n",
    "print(\"EOS_ID (custom):\", EOS_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_CSV)\n",
    "cols = {\"source_text\",\"target_text\",\"split\"}\n",
    "if not cols.issubset(df.columns):\n",
    "    raise ValueError(f\"El CSV debe contener {cols}\")\n",
    "\n",
    "val_df = df[df[\"split\"] == SPLIT].dropna(subset=[\"source_text\",\"target_text\"]).reset_index(drop=True)\n",
    "if len(val_df) == 0:\n",
    "    raise ValueError(f\"No hay filas para split='{SPLIT}'\")\n",
    "if NUM_EXAMPLES < len(val_df):\n",
    "    val_df = val_df.sample(n=NUM_EXAMPLES, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "sources = val_df[\"source_text\"].tolist()\n",
    "targets = val_df[\"target_text\"].tolist()\n",
    "\n",
    "def build_prompt(src: str) -> str:\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "         {\"role\":\"user\",\"content\":USER_PREFIX + str(src)}],\n",
    "        tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "prompts = [build_prompt(s) for s in sources]\n",
    "len(prompts), len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca33c05",
   "metadata": {},
   "source": [
    "## Generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67af34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples(prompts: List[str], n_samples: int) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Devuelve una lista length=len(prompts). Cada elemento es una lista de 'n_samples' resúmenes.\n",
    "    Sin manipular semillas. Muestra progreso total (rondas × batches).\n",
    "    \"\"\"\n",
    "    all_outputs = [list() for _ in range(len(prompts))]\n",
    "    base_cfg = GEN_CFG.copy()\n",
    "    if EOS_ID is not None:\n",
    "        base_cfg[\"eos_token_id\"] = EOS_ID\n",
    "    base_cfg[\"pad_token_id\"] = tokenizer.pad_token_id\n",
    "\n",
    "    total_batches = n_samples * ceil(len(prompts) / max(1, BATCH_GEN))\n",
    "    pbar = tqdm(total=total_batches, desc=\"Generando\", unit=\"batch\")\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        for i in range(0, len(prompts), BATCH_GEN):\n",
    "            j = min(i + BATCH_GEN, len(prompts))\n",
    "            batch_prompts = prompts[i:j]\n",
    "            inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "            gen = model.generate(**inputs, **base_cfg)\n",
    "            cut = inputs[\"input_ids\"].shape[1]\n",
    "            outs = tokenizer.batch_decode(gen[:, cut:], skip_special_tokens=True)\n",
    "            for idx, txt in enumerate(outs):\n",
    "                all_outputs[i + idx].append(txt.strip())\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return all_outputs\n",
    "\n",
    "generated = generate_samples(prompts, SAMPLES_PER_EXAMPLE)\n",
    "assert len(generated) == len(prompts)\n",
    "assert all(len(xs) == SAMPLES_PER_EXAMPLE for xs in generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b650503",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c04cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked(iterable, n):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]\n",
    "\n",
    "rows = []  # filas por ejemplo×muestra\n",
    "for doc_id, (src, tgt, gens) in enumerate(zip(sources, targets, generated)):\n",
    "    # evaluamos por micro-lotes contra la API\n",
    "    pairs_src, pairs_gen = [], []\n",
    "    for sidx, gen_txt in enumerate(gens):\n",
    "        pairs_src.append(src)\n",
    "        pairs_gen.append(gen_txt)\n",
    "\n",
    "    # relevance\n",
    "    rel_scores = []\n",
    "    for c_src, c_gen in zip(chunked(pairs_src, BATCH_METRICS), chunked(pairs_gen, BATCH_METRICS)):\n",
    "        rel_scores.extend(getRelevance(c_src, c_gen, base_url=METRICS_URL, timeout=180.0))\n",
    "\n",
    "    # factuality\n",
    "    fac_scores = []\n",
    "    for c_src, c_gen in zip(chunked(pairs_src, BATCH_METRICS), chunked(pairs_gen, BATCH_METRICS)):\n",
    "        fac_scores.extend(getFactuality(c_src, c_gen, base_url=METRICS_URL, timeout=900.0))\n",
    "\n",
    "    # readability (solo sobre generados)\n",
    "    fkgl_all, smog_all, dale_all = [], [], []\n",
    "    for c_gen in chunked(pairs_gen, BATCH_METRICS):\n",
    "        rd = getReadability(c_gen, base_url=METRICS_URL, timeout=180.0)\n",
    "        fkgl_all.extend(rd[\"fkgl\"]); smog_all.extend(rd[\"smog\"]); dale_all.extend(rd[\"dale_chall\"])\n",
    "\n",
    "    # registrar filas\n",
    "    for sidx in range(len(gens)):\n",
    "        rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sample_idx\": sidx,\n",
    "            \"relevance\": float(rel_scores[sidx]),\n",
    "            \"factuality\": float(fac_scores[sidx]),\n",
    "            \"fkgl\": float(fkgl_all[sidx]),\n",
    "            \"smog\": float(smog_all[sidx]),\n",
    "            \"dale_chall\": float(dale_all[sidx]),\n",
    "        })\n",
    "\n",
    "per_sample_df = pd.DataFrame(rows).sort_values([\"doc_id\",\"sample_idx\"]).reset_index(drop=True)\n",
    "per_sample_df.head(), per_sample_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63098ec",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56416c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_series(x: pd.Series) -> Dict[str, float]:\n",
    "    v = x.dropna().to_numpy()\n",
    "    if v.size == 0:\n",
    "        return {\"mean\": None,\"std\": None,\"min\": None,\"max\": None,\"p25\": None,\"p50\": None,\"p75\": None,\"n\": 0}\n",
    "    return {\n",
    "        \"mean\": float(np.mean(v)),\n",
    "        \"std\":  float(np.std(v, ddof=1)) if v.size > 1 else 0.0,\n",
    "        \"min\":  float(np.min(v)),\n",
    "        \"max\":  float(np.max(v)),\n",
    "        \"p25\":  float(np.percentile(v, 25)),\n",
    "        \"p50\":  float(np.percentile(v, 50)),\n",
    "        \"p75\":  float(np.percentile(v, 75)),\n",
    "        \"n\":    int(v.size),\n",
    "    }\n",
    "\n",
    "# Agregación global\n",
    "stats = {\n",
    "    \"relevance\":   summarize_series(per_sample_df[\"relevance\"]),\n",
    "    \"factuality\":  summarize_series(per_sample_df[\"factuality\"]),\n",
    "    \"fkgl\":        summarize_series(per_sample_df[\"fkgl\"]),\n",
    "    \"smog\":        summarize_series(per_sample_df[\"smog\"]),\n",
    "    \"dale_chall\":  summarize_series(per_sample_df[\"dale_chall\"]),\n",
    "}\n",
    "\n",
    "# Imprimir estadísticas\n",
    "print(\"=== Estadísticas globales ===\")\n",
    "for k, s in stats.items():\n",
    "    print(f\"\\n{k.upper()}: n={s['n']}\")\n",
    "    print(f\"  mean={s['mean']:.4f}  std={s['std']:.4f}  min={s['min']:.4f}  p25={s['p25']:.4f}  p50={s['p50']:.4f}  p75={s['p75']:.4f}  max={s['max']:.4f}\")\n",
    "\n",
    "# Graficar (una figura por métrica)\n",
    "def plot_distribution(series: pd.Series, title: str):\n",
    "    v = series.dropna().to_numpy()\n",
    "    if v.size == 0:\n",
    "        print(f\"[skip] {title}: sin datos\")\n",
    "        return\n",
    "    plt.figure(figsize=(7,4))\n",
    "    # histograma simple\n",
    "    plt.hist(v, bins=30, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"valor\")\n",
    "    plt.ylabel(\"frecuencia\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # boxplot\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.boxplot(v, vert=True, showmeans=True)\n",
    "    plt.title(f\"{title} — boxplot\")\n",
    "    plt.ylabel(\"valor\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_distribution(per_sample_df[\"relevance\"],  \"Relevance\")\n",
    "plot_distribution(per_sample_df[\"factuality\"], \"Factuality\")\n",
    "plot_distribution(per_sample_df[\"fkgl\"],       \"FKGL\")\n",
    "plot_distribution(per_sample_df[\"smog\"],       \"SMOG\")\n",
    "plot_distribution(per_sample_df[\"dale_chall\"], \"Dale-Chall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ee86a",
   "metadata": {},
   "source": [
    "## Experimento en Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "from mlflow.data import from_pandas\n",
    "\n",
    "\n",
    "os.environ[\"DATABRICKS_HOST\"] = KEYS.DATABRICKS_HOST\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = KEYS.DATABRICKS_TOKEN\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(\"/GeneracionDeResumenes/Estadísticas_de_Modelos\")\n",
    "\n",
    "# --- Preparación de artefactos a guardar ---\n",
    "RUN_OUT = Path(OUT_DIR) / \"mlflow_artifacts\"\n",
    "PLOTS_DIR = RUN_OUT / \"plots\"\n",
    "RUN_OUT.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Graficar y guardar (hist + boxplot) para cada métrica\n",
    "def save_distribution(series: pd.Series, title: str, slug: str):\n",
    "    v = series.dropna().to_numpy()\n",
    "    if v.size == 0:\n",
    "        return\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(v, bins=30, alpha=0.7)\n",
    "    plt.title(title); plt.xlabel(\"valor\"); plt.ylabel(\"frecuencia\"); plt.grid(alpha=0.3)\n",
    "    hist_path = PLOTS_DIR / f\"{slug}_hist.png\"\n",
    "    plt.savefig(hist_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.boxplot(v, vert=True, showmeans=True)\n",
    "    plt.title(f\"{title} — boxplot\"); plt.ylabel(\"valor\"); plt.grid(alpha=0.3)\n",
    "    box_path = PLOTS_DIR / f\"{slug}_box.png\"\n",
    "    plt.savefig(box_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "save_distribution(per_sample_df[\"relevance\"],  \"Relevance\",   \"relevance\")\n",
    "save_distribution(per_sample_df[\"factuality\"], \"Factuality\",  \"factuality\")\n",
    "save_distribution(per_sample_df[\"fkgl\"],       \"FKGL\",        \"fkgl\")\n",
    "save_distribution(per_sample_df[\"smog\"],       \"SMOG\",        \"smog\")\n",
    "save_distribution(per_sample_df[\"dale_chall\"], \"Dale-Chall\",  \"dale_chall\")\n",
    "\n",
    "# --- Run name  ---\n",
    "SAFE_MODEL_NAME = str(MODEL_DIR).replace(os.sep, \"__\").replace(\"/\", \"__\").replace(\" \", \"_\")\n",
    "run_name = f\"{SAFE_MODEL_NAME}-eval-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "\n",
    "# --- Iniciar y loggear en MLflow ---\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    # Dataset de entrada (solo split usado en esta evaluación)\n",
    "    try:\n",
    "        ds_val = from_pandas(val_df, source=\"PLS_pairs\", name=f\"PLS_{SPLIT}\")\n",
    "        mlflow.log_input(ds_val, context=\"validation\")\n",
    "    except Exception as e:\n",
    "        print(\"[MLflow] Aviso: no pude registrar dataset de validación:\", e)\n",
    "\n",
    "    # Parámetros de evaluación y generación\n",
    "    mlflow.log_params({\n",
    "        \"model_dir\": SAFE_MODEL_NAME,\n",
    "        \"device\": DEVICE,\n",
    "        \"split\": SPLIT,\n",
    "        \"num_examples\": NUM_EXAMPLES,\n",
    "        \"samples_per_example\": SAMPLES_PER_EXAMPLE,\n",
    "        \"System Prompt\" : SYSTEM_PROMPT,\n",
    "        \"User_Prefix\" : USER_PREFIX,\n",
    "        **{f\"gen_{k}\": v for k, v in GEN_CFG.items()}, # parámetros de generación\n",
    "    })\n",
    "\n",
    "    # Métricas agregadas (prefijos por métrica)\n",
    "    def log_agg(prefix: str, d: dict):\n",
    "        clean = {k: float(v) for k, v in d.items() if isinstance(v, (int, float))}\n",
    "        mlflow.log_metrics({f\"{prefix}_{k}\": v for k, v in clean.items()})\n",
    "        if \"n\" in d:\n",
    "            mlflow.log_metric(f\"{prefix}_n\", int(d[\"n\"]))\n",
    "    for m in (\"relevance\",\"factuality\",\"fkgl\",\"smog\",\"dale_chall\"):\n",
    "        log_agg(m, stats[m])\n",
    "\n",
    "    # Artefactos (CSV + stats + plots + config del adapter si existe)\n",
    "    mlflow.log_artifact(str(per_sample_csv), artifact_path=\"metrics\")\n",
    "    mlflow.log_artifact(str(stats_json_path), artifact_path=\"metrics\")\n",
    "    for p in PLOTS_DIR.glob(\"*.png\"):\n",
    "        mlflow.log_artifact(str(p), artifact_path=\"plots\")\n",
    "\n",
    "    adapter_cfg = Path(MODEL_DIR) / \"adapter_config.json\"\n",
    "    if adapter_cfg.exists():\n",
    "        mlflow.log_artifact(str(adapter_cfg), artifact_path=\"model_info\")\n",
    "\n",
    "    # Marca de tiempo de fin\n",
    "    mlflow.set_tag(\"finished_at\", datetime.utcnow().isoformat() + \"Z\")\n",
    "\n",
    "print(\"MLflow: evaluación registrada. Tracking URI:\", mlflow.get_tracking_uri())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_generacion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
