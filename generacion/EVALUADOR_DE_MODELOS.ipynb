{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, time, random, sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from math import ceil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(str(Path.cwd().parents[0])) \n",
    "import KEYS\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Métricas\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from metricas.metrics_client import getRelevance, getFactuality, getReadability\n",
    "\n",
    "# Reproducibilidad\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \", DEVICE)\n",
    "\n",
    "# Prompts (usa los mismos que en entrenamiento)\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You simplify clinical trial protocol text into a plain-language summary for the general public. \"\n",
    "#     \"Keep to 6–8th grade readability, avoid diagnoses and speculation, no hallucinations, \"\n",
    "#     \"and preserve key facts (objective, population, interventions, outcomes, timelines, safety).\"\n",
    "# )\n",
    "# USER_PREFIX = \"Using the following clinical trial protocol text as input, create a plain language summary.\\n\\n\"\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert scientific writer specialized in simplifying clinical trial protocols. \"\n",
    "    \"Your task is to rewrite technical protocol content into a clear, accurate, and accessible plain-language summary \"\n",
    "    \"for a general audience. Maintain a 8th grade reading level, ensure factual accuracy, and strictly avoid \"\n",
    "    \"hallucinations, interpretations, or medical advice. The summary must clearly cover the study’s objective, population, \"\n",
    "    \"interventions, outcomes, timeline, and safety information, using concise and neutral language.\"\n",
    ")\n",
    "\n",
    "USER_PREFIX = (\n",
    "    \"Below is a section from a clinical trial protocol. Rewrite it as a plain-language summary that is accurate, clear, \"\n",
    "    \"and easy to understand for the public while preserving all essential details.\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb049bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_dir: str, device: str = DEVICE):\n",
    "    model_dir = str(Path(model_dir).resolve())\n",
    "    cfg_path = Path(model_dir) / \"adapter_config.json\"\n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"No existe {cfg_path}\")\n",
    "\n",
    "    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        adapter_cfg = json.load(f)\n",
    "    base = adapter_cfg.get(\"base_model_name_or_path\")\n",
    "    if not base:\n",
    "        raise ValueError(\"adapter_config.json no contiene 'base_model_name_or_path'.\")\n",
    "\n",
    "    # Tokenizer desde el directorio del adapter (incluye el token extra)\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"left\"\n",
    "\n",
    "    # Modelo base\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base,\n",
    "        torch_dtype=torch.float16 if device.startswith(\"cuda\") else torch.float32,\n",
    "        trust_remote_code=True\n",
    "    ).to(device)\n",
    "\n",
    "    #  alinear tamaño de vocab del base al del tokenizer usado en el adapter\n",
    "    base_model.resize_token_embeddings(len(tok))\n",
    "\n",
    "    # Cargar adapters LoRA\n",
    "    model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "    model.eval()\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    # token de fin de oración personalizado\n",
    "    eos_id = None\n",
    "    try:\n",
    "        eid = tok.convert_tokens_to_ids(\"<|sentence_end|>\")\n",
    "        if eid is not None and eid != tok.unk_token_id:\n",
    "            eos_id = eid\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tok, eos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1557a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parámetros de evaluación ===\n",
    "MODEL_DIR = \"SummLlama/outputs/DISLab__SummLlama3.2-3B-FKGD13_CON_Sliding_Window/final\"\n",
    "DATASET_CSV = \"../data/pls_abstract_pairs_with_metrics.csv\"\n",
    "SPLIT = \"test\"                  # 'test' o 'train'\n",
    "NUM_EXAMPLES = 70              # cuántos ejemplos del split\n",
    "SAMPLES_PER_EXAMPLE = 2         # cuántos resúmenes por ejemplo\n",
    "\n",
    "SEED = 44\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# generación \n",
    "GEN_CFG = dict(\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.05,\n",
    ")\n",
    "\n",
    "# Parametros de ventana deslizante\n",
    "WINDOW_TOKENS = 512\n",
    "OVERLAP_TOKENS = 128    # solape entre ventanas\n",
    "\n",
    "# batches\n",
    "BATCH_GEN = 2           # prompts por batch para generate()\n",
    "BATCH_METRICS = 4       # micro-lote por POST a la API de métricas\n",
    "\n",
    "# servicio de métricas\n",
    "METRICS_URL = \"http://127.0.0.1:8000\" # corriendo en local\n",
    "\n",
    "# salida\n",
    "OUT_DIR = \"GENERATED_TEXTS\" \n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "model, tokenizer, EOS_ID = load_model_and_tokenizer(MODEL_DIR, DEVICE)\n",
    "print(\"EOS_ID (custom):\", EOS_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0bb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === utilidades de sliding window (ENTRADA) ===\n",
    "def _template_tokens_cost() -> int:\n",
    "    # mide tokens de template con un dummy corto\n",
    "    dummy = \"X\"\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "         {\"role\":\"user\",\"content\":USER_PREFIX + dummy}],\n",
    "        tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return len(tokenizer(prompt, add_special_tokens=False).input_ids)\n",
    "\n",
    "def windowize_source(text: str, window_tokens: int, overlap_tokens: int) -> List[str]:\n",
    "    ids = tokenizer(text, add_special_tokens=False).input_ids\n",
    "    if len(ids) <= window_tokens:\n",
    "        return [text]\n",
    "    windows, step = [], max(1, window_tokens - overlap_tokens)\n",
    "    for start in range(0, len(ids), step):\n",
    "        end = min(start + window_tokens, len(ids))\n",
    "        chunk = tokenizer.decode(ids[start:end], skip_special_tokens=True).strip()\n",
    "        if chunk:\n",
    "            windows.append(chunk)\n",
    "        if end == len(ids):\n",
    "            break\n",
    "    return windows\n",
    "\n",
    "def build_prompt(src: str) -> str:\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "         {\"role\":\"user\",\"content\":USER_PREFIX + str(src)}],\n",
    "        tokenize=False, add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generación con ventana deslizante en ENTRADA y fusión de parciales ===\n",
    "@torch.no_grad()\n",
    "def generate_samples(sliding_sources: List[List[str]], n_samples: int) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    `sliding_sources`: lista de documentos; cada documento es una lista de ventanas de *source* (texto).\n",
    "    Retorna: lista de documentos; cada doc es lista de 'n_samples' salidas (cada salida es fusión de parciales).\n",
    "    \"\"\"\n",
    "    all_doc_outputs = [list() for _ in range(len(sliding_sources))]\n",
    "    base_cfg = GEN_CFG.copy()\n",
    "    # Usar ambos EOS si existe custom\n",
    "    if EOS_ID is not None:\n",
    "        base_cfg[\"eos_token_id\"] = [tokenizer.eos_token_id, EOS_ID]\n",
    "    else:\n",
    "        base_cfg[\"eos_token_id\"] = tokenizer.eos_token_id\n",
    "    base_cfg[\"pad_token_id\"] = tokenizer.pad_token_id\n",
    "\n",
    "    # Para mostrar progreso: nº de rondas × nº de docs × nº de lotes por doc (aprox por ventanas)\n",
    "    total_windows = sum(len(wins) for wins in sliding_sources)\n",
    "    approx_batches_per_round = ceil(total_windows / max(1, BATCH_GEN))\n",
    "    pbar = tqdm(total=n_samples * approx_batches_per_round, desc=\"Generando : \", unit=\"batch\")\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # generamos documento por documento para poder fusionar ventanas\n",
    "        for doc_idx, wins in enumerate(sliding_sources):\n",
    "            if not wins:\n",
    "                all_doc_outputs[doc_idx].append(\"\")  # doc vacío\n",
    "                continue\n",
    "\n",
    "            # construir prompts por ventana\n",
    "            prompts = [build_prompt(w) for w in wins]\n",
    "\n",
    "            # generar por lotes de ventanas\n",
    "            partials = []\n",
    "            for i in range(0, len(prompts), BATCH_GEN):\n",
    "                j = min(i + BATCH_GEN, len(prompts))\n",
    "                batch_prompts = prompts[i:j]\n",
    "                inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "                gen = model.generate(**inputs, **base_cfg)\n",
    "                cut = inputs[\"input_ids\"].shape[1]\n",
    "                outs = tokenizer.batch_decode(gen[:, cut:], skip_special_tokens=True)\n",
    "                partials.extend([o.strip() for o in outs])\n",
    "                pbar.update(1)\n",
    "\n",
    "            # fusión (idéntica a lo discutido en entrenamiento: concatenación simple)\n",
    "            fused = \"\\n\".join(p for p in partials if p)\n",
    "            all_doc_outputs[doc_idx].append(fused)\n",
    "\n",
    "    pbar.close()\n",
    "    return all_doc_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec59ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_CSV)\n",
    "cols = {\"source_text\",\"target_text\",\"split\"}\n",
    "if not cols.issubset(df.columns):\n",
    "    raise ValueError(f\"El CSV debe contener {cols}\")\n",
    "\n",
    "val_df = df[df[\"split\"] == SPLIT].dropna(subset=[\"source_text\",\"target_text\"]).reset_index(drop=True)\n",
    "if len(val_df) == 0:\n",
    "    raise ValueError(f\"No hay filas para split='{SPLIT}'\")\n",
    "if NUM_EXAMPLES < len(val_df):\n",
    "    val_df = val_df.sample(n=NUM_EXAMPLES, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "sources = val_df[\"source_text\"].tolist()\n",
    "targets = val_df[\"target_text\"].tolist()\n",
    "\n",
    "# Crear ventanas por documento (\n",
    "sliding_sources = [windowize_source(src, WINDOW_TOKENS, OVERLAP_TOKENS) for src in sources]\n",
    "\n",
    "generated = generate_samples(sliding_sources, SAMPLES_PER_EXAMPLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(generated) == len(sliding_sources)\n",
    "assert all(len(xs) == SAMPLES_PER_EXAMPLE for xs in generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489015b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked(iterable, n):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]\n",
    "\n",
    "rows = []  # filas por ejemplo×muestra\n",
    "for doc_id, (src, tgt, gens) in enumerate(zip(sources, targets, generated)):\n",
    "    # evaluamos por micro-lotes contra la API\n",
    "    pairs_src, pairs_gen = [], []\n",
    "    for sidx, gen_txt in enumerate(gens):\n",
    "        pairs_src.append(src)          # siempre contra el documento completo\n",
    "        pairs_gen.append(gen_txt)      # salida final fusionada\n",
    "\n",
    "    # relevance\n",
    "    rel_scores = []\n",
    "    for c_src, c_gen in zip(chunked(pairs_src, BATCH_METRICS), chunked(pairs_gen, BATCH_METRICS)):\n",
    "        rel_scores.extend(getRelevance(c_src, c_gen, base_url=METRICS_URL, timeout=180.0))\n",
    "\n",
    "    # factuality\n",
    "    fac_scores = []\n",
    "    for c_src, c_gen in zip(chunked(pairs_src, BATCH_METRICS), chunked(pairs_gen, BATCH_METRICS)):\n",
    "        fac_scores.extend(getFactuality(c_src, c_gen, base_url=METRICS_URL, timeout=900.0))\n",
    "\n",
    "    # readability (solo sobre generados)\n",
    "    fkgl_all, smog_all, dale_all = [], [], []\n",
    "    for c_gen in chunked(pairs_gen, BATCH_METRICS):\n",
    "        rd = getReadability(c_gen, base_url=METRICS_URL, timeout=180.0)\n",
    "        fkgl_all.extend(rd[\"fkgl\"]); smog_all.extend(rd[\"smog\"]); dale_all.extend(rd[\"dale_chall\"])\n",
    "\n",
    "    # registrar filas\n",
    "    for sidx in range(len(gens)):\n",
    "        rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sample_idx\": sidx,\n",
    "            \"relevance\": float(rel_scores[sidx]),\n",
    "            \"factuality\": float(fac_scores[sidx]),\n",
    "            \"fkgl\": float(fkgl_all[sidx]),\n",
    "            \"smog\": float(smog_all[sidx]),\n",
    "            \"dale_chall\": float(dale_all[sidx]),\n",
    "        })\n",
    "\n",
    "per_sample_df = pd.DataFrame(rows).sort_values([\"doc_id\",\"sample_idx\"]).reset_index(drop=True)\n",
    "per_sample_df.head(), per_sample_df.shape\n",
    "\n",
    "def summarize_series(x: pd.Series) -> Dict[str, float]:\n",
    "    v = x.dropna().to_numpy()\n",
    "    if v.size == 0:\n",
    "        return {\"mean\": None,\"std\": None,\"min\": None,\"max\": None,\"p25\": None,\"p50\": None,\"p75\": None,\"n\": 0}\n",
    "    return {\n",
    "        \"mean\": float(np.mean(v)),\n",
    "        \"std\":  float(np.std(v, ddof=1)) if v.size > 1 else 0.0,\n",
    "        \"min\":  float(np.min(v)),\n",
    "        \"max\":  float(np.max(v)),\n",
    "        \"p25\":  float(np.percentile(v, 25)),\n",
    "        \"p50\":  float(np.percentile(v, 50)),\n",
    "        \"p75\":  float(np.percentile(v, 75)),\n",
    "        \"n\":    int(v.size),\n",
    "    }\n",
    "\n",
    "# Agregación global\n",
    "stats = {\n",
    "    \"relevance\":   summarize_series(per_sample_df[\"relevance\"]),\n",
    "    \"factuality\":  summarize_series(per_sample_df[\"factuality\"]),\n",
    "    \"fkgl\":        summarize_series(per_sample_df[\"fkgl\"]),\n",
    "    \"smog\":        summarize_series(per_sample_df[\"smog\"]),\n",
    "    \"dale_chall\":  summarize_series(per_sample_df[\"dale_chall\"]),\n",
    "}\n",
    "\n",
    "# Imprimir estadísticas\n",
    "print(\"=== Estadísticas globales ===\")\n",
    "for k, s in stats.items():\n",
    "    print(f\"\\n{k.upper()}: n={s['n']}\")\n",
    "    print(f\"  mean={s['mean']:.4f}  std={s['std']:.4f}  min={s['min']:.4f}  p25={s['p25']:.4f}  p50={s['p50']:.4f}  p75={s['p75']:.4f}  max={s['max']:.4f}\")\n",
    "\n",
    "# Graficar (una figura por métrica)\n",
    "def plot_distribution(series: pd.Series, title: str):\n",
    "    v = series.dropna().to_numpy()\n",
    "    if v.size == 0:\n",
    "        print(f\"[skip] {title}: sin datos\")\n",
    "        return\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(v, bins=30, alpha=0.7)\n",
    "    plt.title(title); plt.xlabel(\"valor\"); plt.ylabel(\"frecuencia\"); plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.boxplot(v, vert=True, showmeans=True)\n",
    "    plt.title(f\"{title} — boxplot\"); plt.ylabel(\"valor\"); plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_distribution(per_sample_df[\"relevance\"],  \"Relevance\")\n",
    "plot_distribution(per_sample_df[\"factuality\"], \"Factuality\")\n",
    "plot_distribution(per_sample_df[\"fkgl\"],       \"FKGL\")\n",
    "plot_distribution(per_sample_df[\"smog\"],       \"SMOG\")\n",
    "plot_distribution(per_sample_df[\"dale_chall\"], \"Dale-Chall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7684e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar generacion en CSV: primera columna = source_text, siguientes = gen_1..gen_N\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "SAFE_MODEL_NAME = str(MODEL_DIR).replace(os.sep, \"__\").replace(\"/\", \"__\").replace(\" \", \"_\")\n",
    "cols = [\"source_text\"] + [f\"gen_{i+1}\" for i in range(SAMPLES_PER_EXAMPLE)]\n",
    "rows = []\n",
    "\n",
    "for src, gens in zip(sources, generated):\n",
    "    # `generated` es una lista de listas con SAMPLES_PER_EXAMPLE textos por fila\n",
    "    row = [src] + gens\n",
    "    rows.append(row)\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=cols)\n",
    "out_path = Path(OUT_DIR) / f\"{SAFE_MODEL_NAME}_generations.csv\"\n",
    "df_out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"[saved] {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be2cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MLflow (sin cambios, excepto que loguea lo generado tras sliding) ===\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "from mlflow.data import from_pandas\n",
    "\n",
    "os.environ[\"DATABRICKS_HOST\"] = KEYS.DATABRICKS_HOST\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = KEYS.DATABRICKS_TOKEN\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(\"/GeneracionDeResumenes/Estadísticas_de_Modelos\")\n",
    "\n",
    "RUN_OUT = Path(OUT_DIR) / \"mlflow_artifacts\"\n",
    "PLOTS_DIR = RUN_OUT / \"plots\"\n",
    "RUN_OUT.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_distribution(series: pd.Series, title: str, slug: str):\n",
    "    v = series.dropna().to_numpy()\n",
    "    if v.size == 0:\n",
    "        return\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(v, bins=30, alpha=0.7)\n",
    "    plt.title(title); plt.xlabel(\"valor\"); plt.ylabel(\"frecuencia\"); plt.grid(alpha=0.3)\n",
    "    hist_path = PLOTS_DIR / f\"{slug}_hist.png\"\n",
    "    plt.savefig(hist_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.boxplot(v, vert=True, showmeans=True)\n",
    "    plt.title(f\"{title} — boxplot\"); plt.ylabel(\"valor\"); plt.grid(alpha=0.3)\n",
    "    box_path = PLOTS_DIR / f\"{slug}_box.png\"\n",
    "    plt.savefig(box_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "save_distribution(per_sample_df[\"relevance\"],  \"Relevance\",   \"relevance\")\n",
    "save_distribution(per_sample_df[\"factuality\"], \"Factuality\",  \"factuality\")\n",
    "save_distribution(per_sample_df[\"fkgl\"],       \"FKGL\",        \"fkgl\")\n",
    "save_distribution(per_sample_df[\"smog\"],       \"SMOG\",        \"smog\")\n",
    "save_distribution(per_sample_df[\"dale_chall\"], \"Dale-Chall\",  \"dale_chall\")\n",
    "\n",
    "# per_sample_csv = RUN_OUT / \"per_sample_metrics.csv\"\n",
    "# per_sample_df.to_csv(per_sample_csv, index=False)\n",
    "\n",
    "stats_json_path = RUN_OUT / \"aggregate_stats.json\"\n",
    "with open(stats_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stats, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "SAFE_MODEL_NAME = str(MODEL_DIR).replace(os.sep, \"__\").replace(\"/\", \"__\").replace(\" \", \"_\")\n",
    "run_name = f\"{SAFE_MODEL_NAME}-eval-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    try:\n",
    "        ds_val = from_pandas(val_df, source=\"PLS_pairs\", name=f\"PLS_{SPLIT}\")\n",
    "        mlflow.log_input(ds_val, context=\"validation\")\n",
    "    except Exception as e:\n",
    "        print(\"[MLflow] Aviso: no pude registrar dataset de validación:\", e)\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"model_dir\": SAFE_MODEL_NAME,\n",
    "        \"device\": DEVICE,\n",
    "        \"split\": SPLIT,\n",
    "        \"num_examples\": NUM_EXAMPLES,\n",
    "        \"samples_per_example\": SAMPLES_PER_EXAMPLE,\n",
    "        \"System Prompt\": SYSTEM_PROMPT,\n",
    "        \"User_Prefix\": USER_PREFIX,\n",
    "        **{f\"gen_{k}\": v for k, v in GEN_CFG.items()},\n",
    "        \"window_tokens\": WINDOW_TOKENS,\n",
    "        \"overlap_tokens\": OVERLAP_TOKENS,\n",
    "        \"fusion_mode\": \"concat\",\n",
    "    })\n",
    "\n",
    "    mlflow.set_tag(\n",
    "        \"mlflow.note.content\",\n",
    "        f\"Evaluación con Sliding Window en ENTRADA (concat parciales) — split={SPLIT}. \"\n",
    "        f\"{SAMPLES_PER_EXAMPLE} muestras/doc. GenCfg={GEN_CFG}\"\n",
    "    )\n",
    "\n",
    "    def log_agg(prefix: str, d: dict):\n",
    "        clean = {k: float(v) for k, v in d.items() if isinstance(v, (int, float))}\n",
    "        mlflow.log_metrics({f\"{prefix}_{k}\": v for k, v in clean.items()})\n",
    "        if \"n\" in d: mlflow.log_metric(f\"{prefix}_n\", int(d[\"n\"]))\n",
    "    for m in (\"relevance\",\"factuality\",\"fkgl\",\"smog\",\"dale_chall\"):\n",
    "        log_agg(m, stats[m])\n",
    "\n",
    "    \n",
    "    mlflow.log_artifact(str(stats_json_path), artifact_path=\"metrics\")\n",
    "    for p in PLOTS_DIR.glob(\"*.png\"):\n",
    "        mlflow.log_artifact(str(p), artifact_path=\"plots\")\n",
    "\n",
    "    adapter_cfg = Path(MODEL_DIR) / \"adapter_config.json\"\n",
    "    if adapter_cfg.exists():\n",
    "        mlflow.log_artifact(str(adapter_cfg), artifact_path=\"model_info\")\n",
    "\n",
    "    from datetime import datetime as _dt\n",
    "    mlflow.set_tag(\"finished_at\", _dt.utcnow().isoformat() + \"Z\")\n",
    "\n",
    "print(\"MLflow: evaluación registrada. Tracking URI:\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560758b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "def export_code_cells():\n",
    "    from IPython import get_ipython\n",
    "    cells = get_ipython().user_ns['In']\n",
    "    code = '\\n\\n'.join([c for c in cells if c.strip()])\n",
    "    return Markdown(f'```python\\n{code}\\n```')\n",
    "export_code_cells()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_generacion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
