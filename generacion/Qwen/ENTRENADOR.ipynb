{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ada16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Generacion-de-resumenes.-medicos\\.venv_generacion\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | GPU: NVIDIA GeForce RTX 3090\n",
      "Model: meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, time, json, random, csv, gc, re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "sys.path.append(str(Path.cwd().parents[1])) \n",
    "import KEYS\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    ")\n",
    "from transformers.trainer_callback import TrainerCallback, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURACIÓN\n",
    "# ----------------------------\n",
    "MODEL_ID = os.getenv(\"MODEL_ID\", \"meta-llama/Llama-3.2-3B-Instruct\")   \n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)                                  \n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CUT_OFF_LEN     = int(os.getenv(\"CUT_OFF_LEN\", \"1024\"))    # longitud máxima del texto de entrada (tokens)\n",
    "MAX_NEW_TOKENS  = int(os.getenv(\"MAX_NEW_TOKENS\", \"512\"))  # límite de tokens que puede generar el modelo\n",
    "TEMPERATURE     = float(os.getenv(\"TEMPERATURE\", \"0.2\"))   # controla la aleatoriedad (más bajo = más determinista)\n",
    "TOP_P           = float(os.getenv(\"TOP_P\", \"0.95\"))        # limita la generación a los tokens más probables (≈TOP_P % de probabilidad acumulada)\n",
    "N_TRAIN         = int(os.getenv(\"N_TRAIN\", \"1500\"))        # número de muestras de entrenamiento (subconjunto)\n",
    "EVAL_STEPS      = int(os.getenv(\"EVAL_STEPS\", \"10\"))       # cada cuántos pasos se evalúa el modelo\n",
    "LOG_STEPS       = int(os.getenv(\"LOG_STEPS\", \"10\"))        # cada cuántos pasos se registran métricas (logs)\n",
    "SAVE_STEPS      = int(os.getenv(\"SAVE_STEPS\", \"50\"))       # cada cuántos pasos se guarda un checkpoint\n",
    "NUM_EPOCHS      = int(os.getenv(\"NUM_EPOCHS\", \"3\"))        # número total de épocas de entrenamiento\n",
    "BATCH_TRAIN     = int(os.getenv(\"BATCH_TRAIN\", \"4\"))       # tamaño de lote (batch) para entrenamiento\n",
    "BATCH_EVAL      = int(os.getenv(\"BATCH_EVAL\", \"4\"))        # tamaño de lote (batch) para validación\n",
    "GRAD_ACC_STEPS  = int(os.getenv(\"GRAD_ACC_STEPS\", \"16\"))   # pasos para acumular gradientes (simula batch grande)\n",
    "LR              = float(os.getenv(\"LR\", \"1e-4\"))           # tasa de aprendizaje\n",
    "WARMUP_RATIO    = float(os.getenv(\"WARMUP_RATIO\", \"0.05\")) # fracción inicial del entrenamiento usada para warmup\n",
    "SEED            = int(os.getenv(\"SEED\", \"99\"))             # semilla aleatoria para reproducibilidad\n",
    "EVAL_SUBSET_SIZE = int(os.getenv(\"EVAL_SUBSET_SIZE\", \"50\"))# Tamaño del subconjunto de validación (<=0 = desactivar y usar todo)\n",
    "\n",
    "# LoRA \n",
    "LORA_R          = int(os.getenv(\"LORA_R\", \"16\"))           # rango de la descomposición de matrices\n",
    "LORA_ALPHA      = int(os.getenv(\"LORA_ALPHA\", \"32\"))       # escala del aprendizaje en capas LoRA\n",
    "LORA_DROPOUT    = float(os.getenv(\"LORA_DROPOUT\", \"0.05\")) # probabilidad de dropout en LoRA\n",
    "\n",
    "# Sliding Window\n",
    "USE_SLIDING_WINDOW = os.getenv(\"USE_SLIDING_WINDOW\", \"0\") == \"1\"  # 1=on, 0=off\n",
    "SW_WINDOW_TOKS    = int(os.getenv(\"SW_WINDOW_TOKS\", \"512\"))       # tamaño de ventana en tokens\n",
    "SW_OVERLAP_TOKS   = int(os.getenv(\"SW_OVERLAP_TOKS\", \"128\"))      # solapamiento en tokens\n",
    "SW_FUSION_MODE    = os.getenv(\"SW_FUSION_MODE\", \"concat\")         # por ahora: 'concat'\n",
    "\n",
    "# In Context Learning (One-shot/Few-shot) \n",
    "USE_FEWSHOT    = os.getenv(\"USE_FEWSHOT\", \"0\") == \"1\"      # Flag para indicar si se usa ICL o no\n",
    "FEWSHOT_K      = int(os.getenv(\"FEWSHOT_K\", \"2\"))          # Cantidad de ejemplos a utilizar\n",
    "FEWSHOT_SEED   = int(os.getenv(\"FEWSHOT_SEED\", str(SEED))) # semilla aleatoria para reproducibilidad\n",
    "ICL_MODE = \"zero-shot\"\n",
    "if USE_FEWSHOT:\n",
    "    ICL_MODE = \"one-shot\" if FEWSHOT_K == 1 else f\"few-shot:{FEWSHOT_K}\"\n",
    "             \n",
    "# Métricas\n",
    "LOSS_WEIGHTS    = [0.2, 0.25, 0.4, 0.0, 0.1]               # Peso ponderado de las métricas en el entrenamiento\n",
    "\n",
    "# Salidas\n",
    "SAFE_MODEL_NAME = MODEL_ID.replace(\"/\", \"__\")\n",
    "OUTPUT_DIR   = os.getenv(\"OUTPUT_DIR\", f\"outputs/{SAFE_MODEL_NAME}-focus-FKGD9\") # <----Cambiar para cada entrenamiento diferente que se quiera almacenar\n",
    "METRICS_CSV  = f\"{OUTPUT_DIR}/train_val_metrics.csv\"\n",
    "PLOT_PATH    = f\"{OUTPUT_DIR}/loss_curves.png\"\n",
    "FINAL_DIR    = f\"{OUTPUT_DIR}/final\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_DIR, exist_ok=True)\n",
    "\n",
    "# Otros\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "print(\"Device:\", DEVICE, \"| GPU:\", torch.cuda.get_device_name(0) if DEVICE==\"cuda\" else \"CPU\")\n",
    "print(\"Model:\", MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc838e5",
   "metadata": {},
   "source": [
    "## Configurar el wrapper de métricas y funcion de pérdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb516b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca la carpeta 'metricas' hacia arriba y añade a sys.path\n",
    "HERE = Path.cwd()\n",
    "found = None\n",
    "for p in [HERE] + list(HERE.parents):\n",
    "    if (p / \"metricas\").is_dir():\n",
    "        found = p\n",
    "        break\n",
    "if not found:\n",
    "    raise RuntimeError(\"No se encontró la carpeta 'metricas' en ningún ancestro.\")\n",
    "if str(found) not in sys.path:\n",
    "    sys.path.insert(0, str(found))\n",
    "\n",
    "from metricas.metrics_client import getLoss  # usa http://localhost:8000\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_composite_loss(model, tokenizer, val_df, sample_size=None):\n",
    "    \"\"\"\n",
    "    Evalúa pérdidas compuestas generando salidas:\n",
    "      - Si USE_SLIDING_WINDOW=True: genera por ventana (del SOURCE) y fusiona outputs (concat).\n",
    "      - Si USE_SLIDING_WINDOW=False: genera una sola vez (comportamiento previo).\n",
    "      - Usa subconjunto de validación definido por sample_size o EVAL_SUBSET_SIZE.\n",
    "    \"\"\"\n",
    "    if len(val_df) == 0:\n",
    "        return None\n",
    "\n",
    "    # Determinar tamaño de muestra efectivo\n",
    "    k = sample_size if (sample_size and sample_size > 0) else EVAL_SUBSET_SIZE\n",
    "    k = max(1, min(k if k and k > 0 else len(val_df), len(val_df)))\n",
    "\n",
    "    # Submuestreo reproducible\n",
    "    step = int(getattr(model, \"global_step\", 0) or 0)\n",
    "    seed = SEED + step\n",
    "    sub = val_df.sample(n=k, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # Construir prompts (con ICL) según política de windowing\n",
    "    prompts = []\n",
    "    ref_sources = []\n",
    "    for s in sub[\"source_text\"].tolist():\n",
    "        src = str(s)\n",
    "\n",
    "        if USE_SLIDING_WINDOW:\n",
    "            wins = windowize_source(src, tokenizer, SW_WINDOW_TOKS, SW_OVERLAP_TOKS)\n",
    "            if not wins:\n",
    "                wins = [src]\n",
    "            prompts.append((\"WINDOWED\", wins))\n",
    "        else:\n",
    "            chat = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\",   \"content\": USER_PREFIX + src},\n",
    "            ]\n",
    "            p = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "            prompts.append((\"SINGLE\", p))\n",
    "\n",
    "        ref_sources.append(src)\n",
    "\n",
    "    # Generación\n",
    "    gens = []\n",
    "    for kind, payload in prompts:\n",
    "        if kind == \"SINGLE\":\n",
    "            inputs = tokenizer(payload, return_tensors=\"pt\", padding=True, truncation=True, max_length=CUT_OFF_LEN).to(DEVICE)\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=True,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                eos_token_id=STOP_EOS_IDS,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                no_repeat_ngram_size=3,\n",
    "                repetition_penalty=1.05,\n",
    "            )\n",
    "            out = tokenizer.batch_decode(gen[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)[0]\n",
    "            gens.append(out.strip())\n",
    "        else:\n",
    "            pieces = []\n",
    "            for win in payload:\n",
    "                chat = [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\",   \"content\": USER_PREFIX + win},\n",
    "                ]\n",
    "                p = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "                inputs = tokenizer(p, return_tensors=\"pt\", padding=True, truncation=True, max_length=CUT_OFF_LEN).to(DEVICE)\n",
    "                gen = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    do_sample=True,\n",
    "                    temperature=TEMPERATURE,\n",
    "                    top_p=TOP_P,\n",
    "                    eos_token_id=STOP_EOS_IDS,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    repetition_penalty=1.05,\n",
    "                )\n",
    "                seg = tokenizer.batch_decode(gen[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)[0]\n",
    "                pieces.append(seg.strip())\n",
    "            gens.append(\" \".join(pieces).strip())\n",
    "\n",
    "    # Calcular pérdida compuesta\n",
    "    losses = getLoss(\n",
    "        ref_sources,\n",
    "        sub[\"target_text\"].tolist(),\n",
    "        gens,\n",
    "        weights=LOSS_WEIGHTS\n",
    "    )\n",
    "\n",
    "    if isinstance(losses, list):\n",
    "        return float(sum(losses) / len(losses))\n",
    "    return float(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb01dd",
   "metadata": {},
   "source": [
    "## Carga y repartición de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed168056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1500 pares \n",
      "Val:   218 pares\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = os.getenv(\"CSV_PATH\", \"../../data/pls_abstract_pairs_with_metrics.csv\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Split en val/test\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "if N_TRAIN:\n",
    "    train_df = train_df.sample(n=min(N_TRAIN, len(train_df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df)} pares \\nVal:   {len(val_df)} pares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11069e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset de Evaluación\n",
    "\n",
    "class SubsetEvalTrainer(Trainer):\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        ds = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        if EVAL_SUBSET_SIZE and EVAL_SUBSET_SIZE > 0 and len(ds) > EVAL_SUBSET_SIZE:\n",
    "            step = getattr(self.state, \"global_step\", 0) or 0\n",
    "            base_seed = self.args.seed if self.args.seed is not None else 42\n",
    "            seed = int(base_seed + step)\n",
    "            rng = np.random.default_rng(seed)\n",
    "            idx = rng.choice(len(ds), size=EVAL_SUBSET_SIZE, replace=False).tolist()\n",
    "            ds = ds.select(idx)\n",
    "        return super().get_eval_dataloader(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574690c",
   "metadata": {},
   "source": [
    "## Definir Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d000783",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You simplify clinical trial protocol text into a plain-language summary for the general public. \"\n",
    "    \"Keep to 6–8th grade readability, avoid diagnoses and speculation, no hallucinations, \"\n",
    "    \"and preserve key facts (objective, population, interventions, outcomes, timelines, safety).\"\n",
    ")\n",
    "USER_PREFIX = \"Using the following clinical trial protocol text as input, create a plain language summary.\\n\\n\"\n",
    "\n",
    "# def _fuse_generations(gens: list[str], mode: str = \"concat\") -> str:\n",
    "#     \"\"\"Fusiona salidas por ventana. Modo simple: concat con normalización ligera.\"\"\"\n",
    "#     if not gens:\n",
    "#         return \"\"\n",
    "#     if mode == \"concat\":\n",
    "#         # Limpieza mínima + unión con saltos\n",
    "#         cleaned = [g.strip() for g in gens if g and g.strip()]\n",
    "#         # Evita duplicados exactos consecutivos\n",
    "#         fused = []\n",
    "#         for g in cleaned:\n",
    "#             if not fused or g != fused[-1]:\n",
    "#                 fused.append(g)\n",
    "#         return \"\\n\".join(fused).strip()\n",
    "#     # Espacio para más estrategias (e.g., RLAIF/rerank/resumen adicional)\n",
    "#     return \" \".join([g.strip() for g in gens if g and g.strip()]).strip()\n",
    "\n",
    "# === Sliding Window helpers (reemplaza chunking) ===\n",
    "def windowize_source(text: str, tokenizer, window_toks: int, overlap_toks: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Divide el 'text' (solo SOURCE) en ventanas por tokens, con solape.\n",
    "    Devuelve ventanas decodificadas (sin tokens especiales).\n",
    "    \"\"\"\n",
    "    s = str(text)\n",
    "    if not s.strip():\n",
    "        return []\n",
    "    ids = tokenizer(s, add_special_tokens=False, return_attention_mask=False, return_tensors=None)[\"input_ids\"]\n",
    "    if not isinstance(ids, list):\n",
    "        ids = list(ids)\n",
    "    if len(ids) == 0:\n",
    "        return []\n",
    "\n",
    "    step = max(1, window_toks - overlap_toks)\n",
    "    windows = []\n",
    "    for start in range(0, len(ids), step):\n",
    "        end = min(start + window_toks, len(ids))\n",
    "        if start >= end:\n",
    "            break\n",
    "        wid = ids[start:end]\n",
    "        win = tokenizer.decode(wid, skip_special_tokens=True)\n",
    "        win = win.strip()\n",
    "        if win:\n",
    "            windows.append(win)\n",
    "        if end == len(ids):\n",
    "            break\n",
    "    return windows\n",
    "\n",
    "def fuse_windows(windows: list[str], mode: str = \"concat\") -> str:\n",
    "    \"\"\"\n",
    "    Fusión simple: concatena ventanas con doble salto de línea.\n",
    "    \"\"\"\n",
    "    if not windows:\n",
    "        return \"\"\n",
    "    if mode == \"concat\":\n",
    "        return \"\\n\\n\".join(w.strip() for w in windows if w.strip())\n",
    "    # Espacio para futuros modos (p.ej. 'vote', 'map-reduce', etc.)\n",
    "    return \"\\n\\n\".join(windows)\n",
    "\n",
    "\n",
    "def build_chat(src: str, tgt: str | None = None, fewshots: list[dict] | None = None):\n",
    "    \"\"\"\n",
    "    Construye el prompt en formato chat.\n",
    "    - Si `fewshots` se pasa, debe ser una lista de dicts con claves 'src' y 'tgt'.\n",
    "    - Si `tgt` es None, NO se añade el turno del assistant (modo inferencia).\n",
    "      Si `tgt` es string, se añade y se cierra con <|sentence_end|>.\n",
    "    \"\"\"\n",
    "    msgs = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "    if fewshots:\n",
    "        for ex in fewshots:\n",
    "            msgs.append({\"role\": \"user\",      \"content\": USER_PREFIX + str(ex[\"src\"])})\n",
    "            msgs.append({\"role\": \"assistant\", \"content\": str(ex[\"tgt\"]).rstrip() + \" <|sentence_end|>\"})\n",
    "\n",
    "    msgs.append({\"role\": \"user\", \"content\": USER_PREFIX + str(src)})\n",
    "\n",
    "    if tgt is not None:\n",
    "        msgs.append({\"role\": \"assistant\", \"content\": str(tgt).rstrip() + \" <|sentence_end|>\"})\n",
    "\n",
    "    return msgs\n",
    "\n",
    "def encode_supervised(batch, tokenizer):\n",
    "    \"\"\"\n",
    "    - Si USE_SLIDING_WINDOW=True: aplica windowing sobre el SOURCE, fusiona con 'concat'\n",
    "      y usa ese 'src_fused' en el prompt.\n",
    "    - Si USE_SLIDING_WINDOW=False: usa el source completo.\n",
    "    - Garantiza que el EOS del assistant (<|sentence_end|>) entre en el presupuesto\n",
    "      recortando SOLO el source.\n",
    "    - No añade EOS en el user/source; SOLO en el assistant (vía build_chat).\n",
    "    \"\"\"\n",
    "    out = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    max_len = CUT_OFF_LEN\n",
    "    eos_id = tokenizer.convert_tokens_to_ids(\"<|sentence_end|>\")\n",
    "\n",
    "    for s, t in zip(batch[\"source_text\"], batch[\"target_text\"]):\n",
    "        src = str(s)\n",
    "        tgt = str(t)\n",
    "\n",
    "        # 1) Sliding window sobre SOURCE (opcional)\n",
    "        if USE_SLIDING_WINDOW:\n",
    "            wins = windowize_source(src, tokenizer, SW_WINDOW_TOKS, SW_OVERLAP_TOKS)\n",
    "            src_fused = fuse_windows(wins, SW_FUSION_MODE) if wins else src\n",
    "        else:\n",
    "            src_fused = src\n",
    "\n",
    "        # 2) Recorte conservador SOLO del SOURCE fusionado, si hace falta\n",
    "        attempt_src = src_fused\n",
    "        for _ in range(6):  # intentos de recorte\n",
    "            # Entrenamiento supervisado: incluimos el turno del assistant con EOS\n",
    "            chat = build_chat(attempt_src, tgt, fewshots=None)\n",
    "            text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "            toks = tokenizer(text, truncation=True, max_length=max_len, padding=False)\n",
    "\n",
    "            # Confirmar que el EOS del assistant quedó dentro del presupuesto\n",
    "            if eos_id in toks[\"input_ids\"]:\n",
    "                out[\"input_ids\"].append(toks[\"input_ids\"])\n",
    "                out[\"attention_mask\"].append(toks[\"attention_mask\"])\n",
    "                break\n",
    "\n",
    "            # Si no entra el EOS, recortamos el SOURCE (manteniendo el target completo)\n",
    "            if len(attempt_src) < 200:\n",
    "                # demasiado corto; aceptamos esta versión para no perder ejemplo\n",
    "                out[\"input_ids\"].append(toks[\"input_ids\"])\n",
    "                out[\"attention_mask\"].append(toks[\"attention_mask\"])\n",
    "                break\n",
    "\n",
    "            # Recorte agresivo del source: 80%\n",
    "            attempt_src = attempt_src[: int(len(attempt_src) * 0.8)]\n",
    "        else:\n",
    "            # Si nunca hizo break (raro), guardar la última versión\n",
    "            out[\"input_ids\"].append(toks[\"input_ids\"])\n",
    "            out[\"attention_mask\"].append(toks[\"attention_mask\"])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9017ec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer & model listos. Device: cuda:0\n",
      "[CONF] SlidingWindow=OFF (window=512, overlap=128, fusion=concat); ICL_MODE=zero-shot\n"
     ]
    }
   ],
   "source": [
    "def load_tokenizer(model_id: str, hf_token: str | None = HF_TOKEN):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True, token=hf_token, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"left\"\n",
    "    return tok\n",
    "\n",
    "def load_causallm(model_id: str, device: str = DEVICE, hf_token: str | None = HF_TOKEN):\n",
    "    # Verifica que sea un modelo de lenguaje causal compatible (no VL)\n",
    "    try:\n",
    "        cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=True, token=hf_token)\n",
    "        arch_ok = any(\"CausalLM\" in a for a in getattr(cfg, \"architectures\", []) or [])\n",
    "    except Exception:\n",
    "        arch_ok = True  # algunos repos no exponen architectures; intentamos cargar de todas formas\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16 if device.startswith(\"cuda\") else torch.float32,\n",
    "        trust_remote_code=True, token=hf_token\n",
    "    ).to(device)\n",
    "\n",
    "    # Si no es CausalLM, esto normalmente falla al forward; advertimos:\n",
    "    if not arch_ok:\n",
    "        print(\"[WARN] El repo no declara arquitectura CausalLM. Si falla el forward, usar un modelo TEXT (no VL).\")\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    return model\n",
    "\n",
    "tokenizer = load_tokenizer(MODEL_ID)\n",
    "model     = load_causallm(MODEL_ID)\n",
    "special = {\"additional_special_tokens\": [\"<|sentence_end|>\"]}\n",
    "added = tokenizer.add_special_tokens(special)\n",
    "if added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tokens de parada y fin de oracion\n",
    "EOS_ID = tokenizer.convert_tokens_to_ids(\"<|sentence_end|>\")\n",
    "STOP_EOS_IDS = list({tokenizer.eos_token_id, EOS_ID})\n",
    "print(\"Tokenizer & model listos. Device:\", next(model.parameters()).device)\n",
    "print(f\"[CONF] SlidingWindow={'ON' if USE_SLIDING_WINDOW else 'OFF'} \"\n",
    "      f\"(window={SW_WINDOW_TOKS}, overlap={SW_OVERLAP_TOKS}, fusion={SW_FUSION_MODE}); \"\n",
    "      f\"ICL_MODE={ICL_MODE}\")\n",
    "\n",
    "# conjunto para few-shots desde training\n",
    "FEWSHOT_EXAMPLES = []\n",
    "if USE_FEWSHOT and len(train_df) > 0 and FEWSHOT_K > 0:\n",
    "    rng = np.random.RandomState(FEWSHOT_SEED)\n",
    "    idx = rng.choice(len(train_df), size=min(FEWSHOT_K, len(train_df)), replace=False)\n",
    "    FEWSHOT_EXAMPLES = [\n",
    "        {\"src\": train_df.iloc[i][\"source_text\"], \"tgt\": train_df.iloc[i][\"target_text\"]}\n",
    "        for i in idx\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22356315",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89bd7c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1500/1500 [00:10<00:00, 144.29 examples/s]\n",
      "Map: 100%|██████████| 218/218 [00:01<00:00, 154.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 1500\n",
      "}) Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 218\n",
      "})\n",
      "[sliding-window] OFF | window=512, overlap=128, fusion='concat'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hf_train = Dataset.from_pandas(train_df[[\"source_text\",\"target_text\"]]).map(\n",
    "    lambda b: encode_supervised(b, tokenizer), batched=True, remove_columns=[\"source_text\",\"target_text\"]\n",
    ")\n",
    "hf_val = Dataset.from_pandas(val_df[[\"source_text\",\"target_text\"]]).map(\n",
    "    lambda b: encode_supervised(b, tokenizer), batched=True, remove_columns=[\"source_text\",\"target_text\"]\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "print(hf_train, hf_val)\n",
    "\n",
    "print(f\"[sliding-window] {'ON' if USE_SLIDING_WINDOW else 'OFF'} | window={SW_WINDOW_TOKS}, overlap={SW_OVERLAP_TOKS}, fusion='{SW_FUSION_MODE}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d797453",
   "metadata": {},
   "source": [
    "## LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a691d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA targets: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\n",
      "Trainable: 24.31M / 3237.07M (0.75%)\n"
     ]
    }
   ],
   "source": [
    "def auto_find_lora_targets(model, extra_patterns=None):\n",
    "    common = {\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "        \"wi\",\"wo\",\"wq\",\"wk\",\"wv\",\"W_pack\",\"query_key_value\"\n",
    "    }\n",
    "    if extra_patterns:\n",
    "        common |= set(extra_patterns)\n",
    "    found = set()\n",
    "    for name, module in model.named_modules():\n",
    "        base = name.split(\".\")[-1]\n",
    "        if base in common:\n",
    "            found.add(base)\n",
    "    if not found:\n",
    "        found = {\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"}\n",
    "    return sorted(found)\n",
    "\n",
    "targets = auto_find_lora_targets(model)\n",
    "print(\"LoRA targets:\", targets)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=targets\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.train()\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable/1e6:.2f}M / {total/1e6:.2f}M ({100*trainable/total:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c185436",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39149ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValCSVLogger(TrainerCallback):\n",
    "    def __init__(self, csv_path=METRICS_CSV):\n",
    "        self.csv_path = csv_path\n",
    "        os.makedirs(os.path.dirname(self.csv_path), exist_ok=True)\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            with open(self.csv_path, \"w\", newline=\"\") as f:\n",
    "                csv.writer(f).writerow([\"step\",\"train_loss\",\"eval_loss\",\"lr\",\"timestamp\",\"composite_loss\"])\n",
    "\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        logs = kwargs.get(\"logs\", {})\n",
    "        step = state.global_step\n",
    "        tl   = logs.get(\"loss\")\n",
    "        el   = logs.get(\"eval_loss\")\n",
    "        lr   = logs.get(\"learning_rate\")\n",
    "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([step, tl, el, lr, time.time(), None])\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        comp = eval_composite_loss(kwargs[\"model\"], tokenizer, val_df, sample_size=EVAL_SUBSET_SIZE)\n",
    "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([state.global_step, None, kwargs[\"metrics\"].get(\"eval_loss\"), None, time.time(), comp])\n",
    "\n",
    "early_stop = EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=1e-4)\n",
    "csv_logger = TrainValCSVLogger(METRICS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8bef3",
   "metadata": {},
   "source": [
    "## PEFT (Ajuste Fino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ec879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aprox. training steps: 72\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/72 21:43 < 1:26:52, 0.01 it/s, Epoch 0.64/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.117500</td>\n",
       "      <td>1.866034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    fp16=(DEVICE==\"cuda\"),\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = SubsetEvalTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.add_callback(csv_logger)\n",
    "trainer.add_callback(early_stop)\n",
    "\n",
    "approx_steps = math.ceil(len(hf_train) / max(1, BATCH_TRAIN) / max(1, GRAD_ACC_STEPS)) * NUM_EPOCHS\n",
    "print(\"Aprox. training steps:\", approx_steps)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90486d4",
   "metadata": {},
   "source": [
    "## Guardado y demo de inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b337f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado final (PEFT adapters + tokenizer)\n",
    "trainer.model.save_pretrained(FINAL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_DIR)\n",
    "print(\"Guardado en:\", FINAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo de inferencia \n",
    "\n",
    "demo_src = val_df.iloc[0][\"source_text\"]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    build_chat(demo_src, tgt=None, fewshots=FEWSHOT_EXAMPLES if USE_FEWSHOT else None),\n",
    "    tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Texto original:\\n\", demo_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670712cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "eos_ids = [tokenizer.eos_token_id] + ([EOS_ID] if EOS_ID is not None else [])\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        eos_token_id=STOP_EOS_IDS,     # usa ambos\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.05,\n",
    "    )\n",
    "\n",
    "decoded = tokenizer.decode(gen[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(\"Texto generado:\\n\")\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f0752",
   "metadata": {},
   "source": [
    "## Gráfica de rendimiento (entrenamiento v.s. validación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.read_csv(METRICS_CSV)\n",
    "for c in (\"step\",\"train_loss\",\"eval_loss\",\"lr\"):\n",
    "    if c in dfm.columns: dfm[c] = pd.to_numeric(dfm[c], errors=\"coerce\")\n",
    "dfm = dfm.dropna(subset=[\"train_loss\",\"eval_loss\",\"lr\"], how=\"all\")\n",
    "dfm = dfm.sort_values(\"step\").groupby(\"step\", as_index=False).last()\n",
    "\n",
    "xs_tr = dfm[\"step\"].to_numpy()\n",
    "ys_tr = dfm[\"train_loss\"].to_numpy()\n",
    "xs_ev = dfm[\"step\"].to_numpy()\n",
    "ys_ev = dfm[\"eval_loss\"].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "if np.isfinite(ys_tr).any(): plt.plot(xs_tr, ys_tr, \"-o\", label=\"train_loss\", lw=2, ms=4)\n",
    "if np.isfinite(ys_ev).any(): plt.plot(xs_ev, ys_ev, \"-o\", label=\"eval_loss\",  lw=2, ms=4)\n",
    "\n",
    "vals = np.concatenate([a[~np.isnan(a)] for a in [ys_tr, ys_ev] if len(a)])\n",
    "if len(vals):\n",
    "    pad = max(1e-4, 0.08*(vals.max()-vals.min()))\n",
    "    plt.ylim(vals.min()-pad, vals.max()+pad)\n",
    "\n",
    "plt.title(f\"Training vs Validation Loss — {SAFE_MODEL_NAME}\")\n",
    "plt.xlabel(\"step\"); plt.ylabel(\"loss\"); plt.title(\"Training vs Validation Loss\")\n",
    "plt.grid(alpha=0.3); plt.legend()\n",
    "plt.savefig(PLOT_PATH, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot guardado en:\", PLOT_PATH)\n",
    "print(\"Métricas CSV:\", METRICS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2c295",
   "metadata": {},
   "source": [
    "## Experimento (mlflow -> Databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c22411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "from mlflow.data import from_pandas\n",
    "\n",
    "# Configuración MLflow\n",
    "os.environ[\"DATABRICKS_HOST\"] = KEYS.DATABRICKS_HOST\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = KEYS.DATABRICKS_TOKEN\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(f\"/GeneracionDeResumenes/Resultados_Modelos\")\n",
    "\n",
    "mlflow.end_run()\n",
    "run_name = f\"{SAFE_MODEL_NAME}-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "DESCRIPTION = \"foco en FKGL = 9\"    ######################## Agregar detallesd e la corrida en particular\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    ds_train = from_pandas(train_df, source=\"Cochrane\", name=\"Cochrane_v1\")\n",
    "    mlflow.set_tag(\"mlflow.note.content\", DESCRIPTION)\n",
    "    mlflow.log_input(ds_train, context=\"training\")\n",
    "    mlflow.log_params({\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"PEFT\":\"LoRa\",\n",
    "        \"sliding_window\": USE_SLIDING_WINDOW,\n",
    "        \"sw_window_toks\": SW_WINDOW_TOKS,\n",
    "        \"sw_overlap_toks\": SW_OVERLAP_TOKS,\n",
    "        \"sw_fusion_mode\": SW_FUSION_MODE,\n",
    "        \"icl_mode\": ICL_MODE,\n",
    "        \"fp16\": (DEVICE==\"cuda\"),\n",
    "        \"cut_off_len\": CUT_OFF_LEN,\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_train\": BATCH_TRAIN,\n",
    "        \"batch_eval\": BATCH_EVAL,\n",
    "        \"grad_acc_steps\": GRAD_ACC_STEPS,\n",
    "        \"lr\": LR,\n",
    "        \"lora_r\": LORA_R,\n",
    "        \"lora_alpha\": LORA_ALPHA,\n",
    "        \"lora_dropout\": LORA_DROPOUT,\n",
    "        \"loss_weights\": json.dumps(LOSS_WEIGHTS),\n",
    "        \"n_train_rows\": len(train_df),\n",
    "        \"n_val_rows\": len(val_df),\n",
    "    })      \n",
    "\n",
    "    # Métricas finales (si existen en CSV)\n",
    "    try:\n",
    "        df_metrics = pd.read_csv(METRICS_CSV)\n",
    "        for c in (\"step\",\"train_loss\",\"eval_loss\"):\n",
    "            if c in df_metrics.columns:\n",
    "                df_metrics[c] = pd.to_numeric(df_metrics[c], errors=\"coerce\")\n",
    "        last_train = df_metrics[\"train_loss\"].dropna().iloc[-1] if \"train_loss\" in df_metrics and df_metrics[\"train_loss\"].notna().any() else None\n",
    "        last_eval  = df_metrics[\"eval_loss\"].dropna().iloc[-1]  if \"eval_loss\" in df_metrics and df_metrics[\"eval_loss\"].notna().any() else None\n",
    "        if last_train is not None: mlflow.log_metric(\"train_loss_last\", float(last_train))\n",
    "        if last_eval  is not None: mlflow.log_metric(\"eval_loss_last\",  float(last_eval))\n",
    "    except Exception as e:\n",
    "        print(\"[MLflow] Aviso: no pude leer métricas CSV:\", e)\n",
    "\n",
    "    # Artefactos\n",
    "    if os.path.exists(METRICS_CSV): mlflow.log_artifact(METRICS_CSV, artifact_path=\"metrics\")\n",
    "    if os.path.exists(PLOT_PATH):    mlflow.log_artifact(PLOT_PATH,    artifact_path=\"plots\")\n",
    "\n",
    "    # Guarda adapters y tokenizer como artefactos\n",
    "    #mlflow.log_artifacts(FINAL_DIR, artifact_path=\"model_final\")\n",
    "\n",
    "print(\"MLflow: experimento registrado. Tracking URI:\", mlflow.get_tracking_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def export_code_cells():\n",
    "    from IPython import get_ipython\n",
    "    cells = get_ipython().user_ns['In']\n",
    "    code = '\\n\\n'.join([c for c in cells if c.strip()])\n",
    "    return Markdown(f'```python\\n{code}\\n```')\n",
    "\n",
    "#export_code_cells()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_generacion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
